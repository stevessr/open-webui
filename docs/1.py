"""
title: Gemini with search & code (Pseudo-streaming) - Robust Version with Detailed Token Calculation
licence: MIT
"""

import json
import logging
import time
import uuid
import re
from typing import AsyncIterable, Optional, Callable, Awaitable, AsyncGenerator, List
import asyncio  # å¼•å…¥ asyncio ç”¨äºè¶…æ—¶å’Œå»¶è¿Ÿ

import httpx
from pydantic import BaseModel, Field

# å‡è®¾ open_webui.env å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨ï¼Œåˆ™ä½¿ç”¨æ ‡å‡†çš„ logging é…ç½®
try:
    from open_webui.env import SRC_LOG_LEVELS

    log_level = SRC_LOG_LEVELS["MAIN"]
except ImportError:
    log_level = logging.INFO

# é…ç½®æ—¥å¿—è®°å½•å™¨
logging.basicConfig(
    level=log_level, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)
logger.setLevel(log_level)


class Pipe:
    """
    ä¸€ä¸ªç”¨äºä¸ Gemini API äº¤äº’çš„ Manifold é£æ ¼ç®¡é“ã€‚
    è¯¥ç®¡é“å¤„ç†æµå¼å“åº”å¹¶æä¾›çŠ¶æ€æ›´æ–°ã€‚
    """

    class Valves(BaseModel):
        # Pydantic æ¨¡å‹ï¼Œç”¨äºé…ç½®ç®¡é“çš„é˜€é—¨ï¼ˆå³è®¾ç½®ï¼‰
        base_url: str = Field(
            default="https://generativelanguage.googleapis.com",
            description="Gemini API çš„åŸºç¡€ URL",
        )
        api_key: str = Field(default="", description="Gemini API å¯†é’¥")
        timeout: int = Field(default=600, description="æ•´ä¸ªè¯·æ±‚çš„è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰")

        # æ–°å¢ï¼šæµç©ºé—²è¶…æ—¶ï¼Œç”¨äºæ£€æµ‹ä¸­æ–­çš„æµ
        stream_idle_timeout: int = Field(
            default=30, description="åœ¨å‡å®šè¿æ¥ä¸­æ–­ä¹‹å‰ï¼Œç­‰å¾…æ–°æ•°æ®çš„æœ€é•¿æ—¶é—´ï¼ˆç§’ï¼‰ã€‚"
        )

        # æ¨¡å‹é…ç½®
        model_id: str = Field(
            default="gemini-2.5-flash",
            description="UI ä¸­ä½¿ç”¨çš„æ¨¡å‹ IDã€‚",
        )
        model_display_name: str = Field(
            default="Gemini 2.5 Flash ç ”ç©¶", description="UI ä¸­æ˜¾ç¤ºçš„æ¨¡å‹åç§°ã€‚"
        )
        api_model: str = Field(
            default="gemini-2.5-flash",
            description="ç”¨äº API è°ƒç”¨çš„å®é™… Gemini æ¨¡å‹åç§°ã€‚",
        )
        # æ€è€ƒé¢„ç®—é…ç½®
        thinking_budget: int = Field(
            default=-1,
            description="Gemini API çš„æ€è€ƒé¢„ç®—ï¼ˆthinkingBudgetï¼‰ã€‚è®¾ç½®ä¸º 0 å…³é—­æ€è€ƒï¼Œ-1 å¼€å¯åŠ¨æ€æ€è€ƒã€‚",
        )
        include_thoughts: bool = Field(
            default=True, description="æ˜¯å¦è¿”å› Gemini çš„æ€è€ƒæ‘˜è¦"
        )
        # è¾“å‡ºå»¶è¿Ÿé…ç½®
        output_delay: float = Field(
            default=0.01,
            description="è¾“å‡ºå»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰ã€‚åœ¨å­—ç¬¦æ¨¡å¼ä¸‹ä¸ºæ¯ä¸ªå­—ç¬¦é—´çš„å»¶è¿Ÿï¼Œåœ¨å—æ¨¡å¼ä¸‹ä¸ºæ¯ä¸ªå—é—´çš„å»¶è¿Ÿã€‚è®¾ç½®ä¸º 0 ä»¥ç¦ç”¨ã€‚",
        )
        # å—çŠ¶è¾“å‡ºé…ç½®
        block_size: int = Field(
            default=10,
            description="æ¯ä¸ªè¾“å‡ºå—çš„å­—ç¬¦æ•°ã€‚å½“å¤§äº 1 æ—¶åˆ†å—è¾“å‡ºï¼Œä¸º 1 æ—¶é€å­—ç¬¦è¾“å‡ºï¼Œå°äº 0 æ—¶æŒ‰ç©ºæ ¼åˆ†å—è¾“å‡ºã€‚",
        )
        # temperature å’Œ top_P é…ç½®
        temperature: float = Field(default=0.7, description="æ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„éšæœºæ€§ã€‚")
        top_p: float = Field(default=0.9, description="æ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„å¤šæ ·æ€§ã€‚")

    def __init__(self):
        self.type = "manifold"
        self.name = ""
        self.valves = self.Valves()
        self.emitter: Optional[Callable[[dict], Awaitable[None]]] = None
        logger.info(f"ç®¡é“ '{self.name}' å·²åˆå§‹åŒ–ã€‚")

    async def emit_status(self, message: str, done: bool = False):
        if self.emitter:
            if message.strip().startswith("<thinking>") and message.strip().endswith(
                "</thinking>"
            ):
                status_payload = {
                    "type": "status",
                    "data": {"description": message, "done": done},
                }
            else:
                status_payload = {
                    "type": "status",
                    "data": {"description": str(message)[:500], "done": done},
                }
            logger.debug(f"å‘é€çŠ¶æ€æ›´æ–°ï¼š{status_payload}")
            await self.emitter(status_payload)

    def get_models(self) -> List[dict]:
        return [
            {
                "id": self.valves.model_id,
                "name": self.valves.model_display_name,
            },
        ]

    def pipes(self) -> List[dict]:
        return self.get_models()

    def split_html_tags(self, text: str) -> List[str]:
        """
        å°†æ–‡æœ¬åˆ†å‰²ä¸º HTML æ ‡ç­¾å’Œæ™®é€šæ–‡æœ¬å—çš„åˆ—è¡¨
        ä¾‹å¦‚ï¼š"Hello <b>world</b>!" -> ["Hello ", "<b>", "world", "</b>", "!"]
        """
        import re

        pattern = r"(<[^>]+>)"
        return re.split(pattern, text)

    async def process_stream(
        self, response: httpx.Response
    ) -> AsyncGenerator[str, None]:
        """
        å¤„ç†æ¥è‡ª Gemini API çš„æœåŠ¡å™¨å‘é€äº‹ä»¶ (SSE) æµï¼Œå¹¶å¢åŠ äº†è¶…æ—¶å’Œå®Œæ•´æ€§æ£€æŸ¥ã€‚
        ï¼ˆå·²ä¿®æ­£ï¼Œå¯å¤„ç†â€œæ€è€ƒâ€å†…å®¹ä¸ºå¸ƒå°”å€¼æˆ–å­—ç¬¦ä¸²çš„æƒ…å†µï¼‰
        """
        logger.info("å¼€å§‹å¤„ç† API å“åº”æµã€‚")
        finish_reason_received = False
        stream_iterator = response.aiter_lines()
        content_yielded = False

        try:
            while True:
                try:
                    line = await asyncio.wait_for(
                        stream_iterator.__anext__(),
                        timeout=self.valves.stream_idle_timeout,
                    )

                    if not line.strip() or not line.startswith("data: "):
                        continue

                    line = line[6:]

                    try:
                        chunk = json.loads(line)
                        logger.debug(f"æ”¶åˆ°å¹¶è§£æäº†æ•°æ®å—ï¼š{chunk}")

                        if "error" in chunk:
                            error_detail = chunk.get("error", {}).get(
                                "message", "æµä¸­å‘ç”ŸæœªçŸ¥é”™è¯¯"
                            )
                            error_msg = f"ğŸš¨ Gemini API é”™è¯¯ï¼š{error_detail}"
                            logger.error(error_msg)
                            await self.emit_status(error_msg, done=True)
                            yield error_msg
                            return

                        if "candidates" in chunk:
                            for candidate in chunk.get("candidates", []):
                                if (
                                    "content" in candidate
                                    and "parts" in candidate["content"]
                                ):
                                    for part in candidate["content"]["parts"]:
                                        is_thought_part = part.get("thought") is True
                                        text_content = part.get("text", "")

                                        if not text_content.strip():
                                            continue

                                        if is_thought_part:
                                            thought_text = text_content.strip()
                                            quoted_lines = [
                                                f"> {line}"
                                                for line in thought_text.splitlines()
                                            ]
                                            quoted_thought = "\n".join(quoted_lines)
                                            thought_msg = (
                                                f"<think>{quoted_thought}</think>"
                                            )
                                            yield thought_msg
                                            content_yielded = True
                                        else:
                                            yield text_content
                                            content_yielded = True

                        # --- æ ¸å¿ƒä¿®æ”¹å¼€å§‹ï¼šåˆ†ç¦»æ€è€ƒ/å·¥å…·å’Œå…¶ä»–éƒ¨åˆ†çš„ Token è®¡ç®— ---
                        if usage_metadata := chunk.get("usageMetadata"):
                            usage_parts = []
                            prompt_tokens = usage_metadata.get("promptTokenCount", 0)
                            candidates_tokens = usage_metadata.get(
                                "candidatesTokenCount", 0
                            )
                            total_tokens = usage_metadata.get("totalTokenCount", 0)

                            # æ ¹æ®æ–‡æ¡£ï¼Œè¿™äº›æ˜¯å¯èƒ½å‡ºç°çš„ä¸â€œæ€è€ƒâ€ç›¸å…³çš„ Token å­—æ®µã€‚
                            thoughts_tokens = usage_metadata.get(
                                "thoughtsTokenCount", 0
                            )
                            tool_use_tokens = usage_metadata.get("toolUseTokenCount", 0)
                            grounding_tokens = usage_metadata.get(
                                "groundingTokenCount", 0
                            )

                            # å°†æ‰€æœ‰éå†…å®¹ç”Ÿæˆçš„ Token åŠ æ€»
                            thinking_and_tool_tokens = (
                                thoughts_tokens + tool_use_tokens + grounding_tokens
                            )

                            # ä»å€™é€‰ Token æ€»æ•°ä¸­å‡å»æ€è€ƒ/å·¥å…· Tokenï¼Œå¾—åˆ°çº¯æ–‡æœ¬è¾“å‡º Token
                            output_text_tokens = (
                                candidates_tokens - thinking_and_tool_tokens
                            )

                            usage_parts.append(f"è¾“å…¥ï¼š{prompt_tokens} tokens")

                            # ä»…å½“çº¯æ–‡æœ¬è¾“å‡º Token å¤§äº 0 æ—¶æ˜¾ç¤º
                            if output_text_tokens > 0:
                                usage_parts.append(
                                    f"è¾“å‡º (å†…å®¹): {output_text_tokens} tokens"
                                )

                            # ä»…å½“æ€è€ƒ/å·¥å…· Token å¤§äº 0 æ—¶æ˜¾ç¤º
                            if thinking_and_tool_tokens > 0:
                                usage_parts.append(
                                    f"è¾“å‡º (æ€è€ƒ/å·¥å…·): {thinking_and_tool_tokens} tokens"
                                )

                            usage_parts.append(f"æ€»è®¡ï¼š{total_tokens} tokens")

                            usage_msg = (
                                f"ç”¨é‡ä¿¡æ¯ï¼š{', '.join(usage_parts)}"
                                if usage_parts
                                else "ç”¨é‡ä¿¡æ¯å¯ç”¨"
                            )
                            logger.info(usage_msg)
                            await self.emit_status(usage_msg, done=False)
                        # --- æ ¸å¿ƒä¿®æ”¹ç»“æŸ ---

                        if finish_reason := chunk.get("candidates", [{}])[0].get(
                            "finishReason"
                        ):
                            logger.info(f"ä» API æ”¶åˆ°å®ŒæˆåŸå› ï¼š{finish_reason}")
                            finish_reason_received = True

                    except json.JSONDecodeError:
                        logger.warning(f"è§£ç  JSON è¡Œå¤±è´¥ï¼š{line}. è·³è¿‡æ­¤è¡Œã€‚")
                        await self.emit_status(
                            f"è­¦å‘Šï¼šæ— æ³•è§£æä¸€ä¸ªæ•°æ®å—ã€‚å¯èƒ½å­˜åœ¨æ ¼å¼é—®é¢˜ã€‚", done=False
                        )
                    except (KeyError, IndexError) as e:
                        logger.debug(
                            f"æ— æ³•ä»æ•°æ®å—ä¸­æå–æ–‡æœ¬æˆ–å…ƒæ•°æ®ï¼š{line}. é”™è¯¯ï¼š{e}. è·³è¿‡æ­¤å—ã€‚"
                        )
                        await self.emit_status(
                            f"è­¦å‘Šï¼šæ¥æ”¶åˆ°æœªçŸ¥æ ¼å¼çš„æ•°æ®å—ã€‚", done=False
                        )

                except StopAsyncIteration:
                    logger.info("å“åº”æµæ­£å¸¸ç»“æŸã€‚")
                    break
                except asyncio.TimeoutError:
                    error_msg = f"ğŸš¨ æµè¶…æ—¶ï¼šåœ¨ {self.valves.stream_idle_timeout} ç§’å†…æœªæ”¶åˆ°æ–°æ•°æ®ï¼Œè¿æ¥å¯èƒ½å·²ä¸­æ–­ã€‚"
                    logger.error(error_msg)
                    await self.emit_status(error_msg, done=True)
                    yield error_msg
                    return

        finally:
            if not finish_reason_received:
                warning_msg = "è­¦å‘Šï¼šAPI å“åº”æµå·²ç»“æŸï¼Œä½†æœªæ”¶åˆ°æ˜ç¡®çš„å®Œæˆä¿¡å·ï¼ˆfinishReasonï¼‰ã€‚è¿™å¯èƒ½è¡¨ç¤ºæµè¢«æ„å¤–ä¸­æ–­æˆ–æœªå®Œå…¨å‘é€ã€‚"
                logger.warning(warning_msg)
                await self.emit_status(warning_msg, done=True)
            elif not content_yielded and finish_reason_received:
                logger.debug(
                    "Stream ended with finish reason but no text content was yielded."
                )

        logger.info("å“åº”æµå¤„ç†å®Œæ¯•ã€‚")

    async def get_request_stream(
        self, messages: list, model_name: str
    ) -> AsyncGenerator[str, None]:
        """æ„å»ºè¯·æ±‚å¹¶ä» Gemini API æµå¼ä¼ è¾“å“åº”ã€‚"""
        api_model = self.valves.api_model
        logger.info(
            f"ä¸º UI æ¨¡å‹ '{model_name}' å‡†å¤‡è¯·æ±‚ï¼Œä½¿ç”¨ API æ¨¡å‹ '{api_model}'ã€‚"
        )

        gemini_contents = []

        for msg in messages:
            role = "user" if msg.get("role") == "user" else "model"
            content = msg.get("content")
            parts = []
            if isinstance(content, str):
                parts.append({"text": content})
            elif isinstance(content, list):
                for part in content:
                    part_type = part.get("type")
                    if part_type == "text":
                        parts.append({"text": part.get("text", "")})
                    elif part_type == "image_url":
                        image_url = part.get("image_url", {}).get("url", "")
                        if (
                            image_url.startswith("data:image")
                            and ";base64," in image_url
                        ):
                            try:
                                header, encoded_data = image_url.split(",", 1)
                                mime_type = header.split(":", 1)[1].split(";", 1)[0]
                                parts.append(
                                    {
                                        "inlineData": {
                                            "mimeType": mime_type,
                                            "data": encoded_data,
                                        }
                                    }
                                )
                            except (ValueError, IndexError) as e:
                                logger.error(
                                    f"Error parsing image data URI: {e}. Skipping image part."
                                )

                        else:
                            logger.warning(
                                f"Unsupported image URL format. Only 'data:image' URIs are supported."
                            )
            if parts:
                gemini_contents.append({"role": role, "parts": parts})

        if gemini_contents and gemini_contents[-1]["role"] == "model":
            gemini_contents.append({"role": "user", "parts": [{"text": "Continue"}]})
            logger.warning(
                "Added a dummy 'user' turn to continue the conversation after a 'model' turn."
            )

        gemini_tools = [
            {"googleSearch": {}},
            {"code_execution": {}},
            {"url_context": {}},
        ]

        data = {
            "contents": gemini_contents,
            "tools": gemini_tools,
            "generationConfig": {
                "temperature": self.valves.temperature,
                "topP": self.valves.top_p,
                "thinkingConfig": {
                    "includeThoughts": self.valves.include_thoughts,
                    "thinkingBudget": self.valves.thinking_budget,
                },
            },
        }

        url = f"/v1beta/models/{api_model}:streamGenerateContent?key={self.valves.api_key}&alt=sse"

        try:
            async with httpx.AsyncClient(
                base_url=self.valves.base_url,
                trust_env=True,
                timeout=self.valves.timeout,
            ) as client:
                await self.emit_status(
                    f"æ­£åœ¨å‘ Gemini æ¨¡å‹å‘é€è¯·æ±‚ï¼š{api_model}", done=False
                )
                async with client.stream("POST", url, json=data) as response:
                    if response.status_code != 200:
                        error_content = await response.aread()
                        error_message = f"ğŸš¨ Gemini API é”™è¯¯ï¼š{response.status_code} - {error_content.decode()}"
                        await self.emit_status(error_message, done=True)
                        yield error_message
                        return

                    async for content in self.process_stream(response):
                        yield content

        except httpx.ConnectError as e:
            error_msg = f"ğŸš¨ è¿æ¥é”™è¯¯ï¼šæ— æ³•è¿æ¥åˆ° {self.valves.base_url}ã€‚è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–åŸºç¡€ URLã€‚ {e}"
            logger.exception(error_msg)
            await self.emit_status(error_msg, done=True)
            yield error_msg
        except httpx.TimeoutException:
            error_msg = (
                f"ğŸš¨ è¯·æ±‚è¶…æ—¶ï¼šå¯¹ Gemini API çš„è¯·æ±‚åœ¨ {self.valves.timeout} ç§’åè¶…æ—¶ã€‚"
                f"è¯·æ£€æŸ¥ç½‘ç»œæˆ–å¢åŠ ç®¡é“è¶…æ—¶è®¾ç½®ã€‚"
            )
            logger.error(error_msg)
            await self.emit_status(error_msg, done=True)
            yield error_msg
        except Exception as e:
            error_msg = f"ğŸš¨ å‘ç”Ÿæ„å¤–é”™è¯¯ï¼š{e}"
            logger.exception(f"åœ¨ get_request_stream ä¸­å‘ç”Ÿæ„å¤–é”™è¯¯ï¼š{e}")
            await self.emit_status(error_msg, done=True)
            yield error_msg

    async def pipe(
        self,
        body: dict,
        __user__: Optional[dict] = None,
        __event_emitter__: Optional[Callable[[dict], Awaitable[None]]] = None,
        __event_call__: Optional[Callable[[dict], Awaitable[dict]]] = None,
    ) -> AsyncGenerator[str, None]:
        """
        ç®¡é“çš„ä¸»å…¥å£ç‚¹ã€‚
        å®ƒéªŒè¯è¯·æ±‚ï¼Œè°ƒç”¨ Gemini APIï¼Œå¹¶ä»¥ä¼ªæµå¼ï¼ˆé€å­—ï¼‰çš„æ–¹å¼è¿”å›å“åº”ã€‚
        """
        self.emitter = __event_emitter__
        request_id = str(uuid.uuid4())
        logger.info(f"[{request_id}] ç®¡é“å¼€å§‹å¤„ç†æ–°è¯·æ±‚ã€‚")
        logger.debug(f"[{request_id}] æ”¶åˆ°çš„è¯·æ±‚ä½“ï¼š{body}")

        try:
            await self.emit_status("æ­£åœ¨éªŒè¯è¯·æ±‚è´Ÿè½½...", done=False)
            if not isinstance(body, dict):
                error_msg = "âŒ é”™è¯¯ï¼šè¯·æ±‚ä½“å¿…é¡»æ˜¯æœ‰æ•ˆçš„ JSON å¯¹è±¡ã€‚"
                yield error_msg
                await self.emit_status("é”™è¯¯ï¼šæ— æ•ˆçš„è¯·æ±‚ä½“ã€‚", done=True)
                return

            messages = body.get("messages")
            if not messages or not isinstance(messages, list):
                error_msg = "âŒ é”™è¯¯ï¼šè¯·æ±‚ä½“å¿…é¡»åŒ…å«ä¸€ä¸ªæœ‰æ•ˆçš„ 'messages' åˆ—è¡¨ã€‚"
                yield error_msg
                await self.emit_status(
                    "é”™è¯¯ï¼šç¼ºå°‘æˆ–æ— æ•ˆçš„ 'messages' åˆ—è¡¨ã€‚", done=True
                )
                return

            if not self.valves.api_key:
                error_msg = (
                    "âŒ é”™è¯¯ï¼šGemini API å¯†é’¥æœªè®¾ç½®ã€‚è¯·åœ¨ç®¡é“é…ç½®ä¸­æä¾› API å¯†é’¥ã€‚"
                )
                yield error_msg
                await self.emit_status(error_msg, done=True)
                return

            logger.info(f"[{request_id}] è¯·æ±‚è´Ÿè½½éªŒè¯é€šè¿‡ã€‚")

            model_id = body.get("model", self.valves.model_id)

            await self.emit_status(f"æ­£åœ¨ä½¿ç”¨æ¨¡å‹ '{model_id}' å¼€å§‹ç”Ÿæˆ...", done=False)

            stream_had_error = False
            full_response = ""

            async for chunk in self.get_request_stream(messages, model_id):
                if chunk.startswith("ğŸš¨"):
                    stream_had_error = True
                    yield chunk
                    continue

                full_response += chunk

                # æ ¹æ®é…ç½®é€‰æ‹©è¾“å‡ºæ–¹å¼ï¼šå—çŠ¶è¾“å‡ºã€å­—ç¬¦è¾“å‡ºæˆ–ç©ºæ ¼åˆ†å—è¾“å‡º
                if self.valves.block_size > 1:
                    # å—çŠ¶è¾“å‡ºæ¨¡å¼ - å…ˆåˆ†ç¦» HTML æ ‡ç­¾
                    for chunk_part in self.split_html_tags(chunk):
                        if chunk_part.startswith("<") and chunk_part.endswith(">"):
                            # HTML æ ‡ç­¾ç›´æ¥è¾“å‡ºï¼Œä¸åˆ†å—
                            yield chunk_part
                        else:
                            # æ™®é€šæ–‡æœ¬åˆ†å—è¾“å‡º
                            for i in range(0, len(chunk_part), self.valves.block_size):
                                block = chunk_part[i : i + self.valves.block_size]
                                if block:  # é¿å…è¾“å‡ºç©ºå—
                                    yield block
                                    if self.valves.output_delay > 0:
                                        await asyncio.sleep(self.valves.output_delay)
                elif self.valves.block_size < 0:
                    # æŒ‰ç©ºæ ¼åˆ†å—è¾“å‡ºæ¨¡å¼ - å…ˆåˆ†ç¦» HTML æ ‡ç­¾
                    for chunk_part in self.split_html_tags(chunk):
                        if chunk_part.startswith("<") and chunk_part.endswith(">"):
                            # HTML æ ‡ç­¾ç›´æ¥è¾“å‡ºï¼Œä¸åˆ†å—
                            yield chunk_part
                        else:
                            # æ™®é€šæ–‡æœ¬æŒ‰ç©ºæ ¼åˆ†å—è¾“å‡º
                            # ä¿®å¤ï¼šä½¿ç”¨ re.split(r'(\s+)') è€Œä¸æ˜¯ split()
                            # split() ä¼šä¸¢å¼ƒæ‰€æœ‰ç©ºç™½ç¬¦ï¼ˆåŒ…æ‹¬æ¢è¡Œç¬¦ï¼‰ï¼Œå¯¼è‡´ä»£ç å—å’Œæ®µè½åˆå¹¶ã€‚
                            # re.split(r'(\s+)') ä¼šä¿ç•™åˆ†éš”ç¬¦ï¼ˆå³ç©ºæ ¼ã€æ¢è¡Œç¬¦ç­‰ï¼‰ï¼Œé˜²æ­¢æ ¼å¼ä¸¢å¤±ã€‚
                            parts = re.split(r"(\s+)", chunk_part)
                            for part in parts:
                                if part:  # é¿å…è¾“å‡ºç©ºä¸²
                                    yield part
                                    if self.valves.output_delay > 0:
                                        await asyncio.sleep(self.valves.output_delay)
                else:
                    # åŸæœ‰çš„å­—ç¬¦è¾“å‡ºæ¨¡å¼
                    skip = False
                    for char in chunk:
                        yield char

                        if char == "<":
                            skip = True
                        elif char == ">":
                            skip = False

                        if skip:
                            continue
                        if self.valves.output_delay > 0:
                            await asyncio.sleep(self.valves.output_delay)

            if not full_response and not stream_had_error:
                logger.warning(f"[{request_id}] å“åº”æµç»“æŸä½†æœªæ”¶åˆ°ä»»ä½•æ–‡æœ¬å†…å®¹ã€‚")
                yield ""

            if not stream_had_error:
                logger.info(f"[{request_id}] å†…å®¹ç”ŸæˆæˆåŠŸä¸”æ— é”™è¯¯ã€‚")
                await self.emit_status("ç”Ÿæˆå®Œæˆã€‚", done=True)
            else:
                logger.warning(f"[{request_id}] å†…å®¹ç”ŸæˆæœŸé—´å‘ç”Ÿé”™è¯¯ã€‚")

        except Exception as e:
            error_msg = f"âŒ ç®¡é“ä¸­å‘ç”Ÿæ„å¤–çš„ç³»ç»Ÿé”™è¯¯ï¼š{e}"
            logger.exception(f"[{request_id}] {error_msg}")
            await self.emit_status(f"è‡´å‘½é”™è¯¯ï¼š{e}", done=True)
            yield error_msg

        logger.info(f"[{request_id}] ç®¡é“å¤„ç†ç»“æŸã€‚")
