"""
title: Gemini with search & code (Pseudo-streaming) - Robust Version with Independent Judgment
licence: MIT
"""

import json
import logging
import time
import uuid
from typing import Optional, Callable, Awaitable, AsyncGenerator, List
import asyncio

import httpx
from pydantic import BaseModel, Field

# å‡è®¾ open_webui.env å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨ï¼Œåˆ™ä½¿ç”¨æ ‡å‡†çš„ logging é…ç½®
try:
    from open_webui.env import SRC_LOG_LEVELS

    log_level = SRC_LOG_LEVELS["MAIN"]
except ImportError:
    log_level = logging.INFO

# é…ç½®æ—¥å¿—è®°å½•å™¨
logging.basicConfig(
    level=log_level, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)
logger.setLevel(log_level)


class Pipe:
    """
    ä¸€ä¸ªç”¨äºä¸ Gemini API äº¤äº’çš„ Manifold é£æ ¼ç®¡é“ã€‚
    è¯¥ç®¡é“å¤„ç†æµå¼å“åº”ï¼Œå¹¶åœ¨æ¯è½®ç»“æŸåä½¿ç”¨æ¨¡å‹ç‹¬ç«‹åˆ¤æ–­æ˜¯å¦éœ€è¦ç»§ç»­ç”Ÿæˆã€‚
    """

    class Valves(BaseModel):
        # Pydantic æ¨¡å‹ï¼Œç”¨äºé…ç½®ç®¡é“çš„é˜€é—¨ï¼ˆå³è®¾ç½®ï¼‰
        base_url: str = Field(
            default="https://generativelanguage.googleapis.com",
            description="Gemini API çš„åŸºç¡€ URL",
        )
        api_key: str = Field(default="", description="Gemini API å¯†é’¥")
        timeout: int = Field(default=600, description="æ•´ä¸ªè¯·æ±‚çš„è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰")

        # æ–°å¢ï¼šæµç©ºé—²è¶…æ—¶ï¼Œç”¨äºæ£€æµ‹ä¸­æ–­çš„æµ
        stream_idle_timeout: int = Field(
            default=30, description="åœ¨å‡å®šè¿æ¥ä¸­æ–­ä¹‹å‰ï¼Œç­‰å¾…æ–°æ•°æ®çš„æœ€é•¿æ—¶é—´ï¼ˆç§’ï¼‰ã€‚"
        )

        # æ¨¡å‹é…ç½®
        model_id: str = Field(
            default="gemini-2.5-flash-lite",
            description="UI ä¸­ä½¿ç”¨çš„æ¨¡å‹ IDã€‚",
        )
        model_display_name: str = Field(
            default="Gemini 2.5 Flash lite ç ”ç©¶", description="UI ä¸­æ˜¾ç¤ºçš„æ¨¡å‹åç§°ã€‚"
        )
        api_model: str = Field(
            default="gemini-2.5-flash-lite",
            description="ç”¨äº API è°ƒç”¨çš„å®é™… Gemini æ¨¡å‹åç§°ã€‚",
        )
        # æ€è€ƒé¢„ç®—é…ç½®
        thinking_budget: int = Field(
            default=-1,
            description="Gemini API çš„æ€è€ƒé¢„ç®—ï¼ˆthinkingBudgetï¼‰ã€‚è®¾ç½®ä¸º 0 å…³é—­æ€è€ƒï¼Œ-1 å¼€å¯åŠ¨æ€æ€è€ƒã€‚",
        )
        include_thoughts: bool = Field(
            default=True, description="æ˜¯å¦è¿”å› Gemini çš„æ€è€ƒæ‘˜è¦"
        )
        # è¾“å‡ºå»¶è¿Ÿé…ç½®
        output_delay: float = Field(
            default=0.01,
            description="è¾“å‡ºå»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰ã€‚åœ¨å­—ç¬¦æ¨¡å¼ä¸‹ä¸ºæ¯ä¸ªå­—ç¬¦é—´çš„å»¶è¿Ÿï¼Œåœ¨å—æ¨¡å¼ä¸‹ä¸ºæ¯ä¸ªå—é—´çš„å»¶è¿Ÿã€‚è®¾ç½®ä¸º 0 ä»¥ç¦ç”¨ã€‚",
        )
        # å—çŠ¶è¾“å‡ºé…ç½®
        block_size: int = Field(
            default=10,
            description="æ¯ä¸ªè¾“å‡ºå—çš„å­—ç¬¦æ•°ã€‚å½“å¤§äº 1 æ—¶åˆ†å—è¾“å‡ºï¼Œä¸º 1 æ—¶é€å­—ç¬¦è¾“å‡ºï¼Œå°äº 0 æ—¶æŒ‰ç©ºæ ¼åˆ†å—è¾“å‡ºã€‚",
        )
        # temperature å’Œ top_P é…ç½®
        temperature: float = Field(default=0.7, description="æ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„éšæœºæ€§ã€‚")
        top_p: float = Field(default=0.9, description="æ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„å¤šæ ·æ€§ã€‚")

    def __init__(self):
        self.type = "manifold"
        self.name = "Gemini 2.5 Flash lite ç ”ç©¶"
        self.valves = self.Valves()
        self.emitter: Optional[Callable[[dict], Awaitable[None]]] = None
        logger.info(f"ç®¡é“ '{self.name}' å·²åˆå§‹åŒ–ã€‚")

    async def emit_status(self, message: str, done: bool = False):
        if self.emitter:
            if message.strip().startswith("<thinking>") and message.strip().endswith(
                "</thinking>"
            ):
                status_payload = {
                    "type": "status",
                    "data": {"description": message, "done": done},
                }
            else:
                status_payload = {
                    "type": "status",
                    "data": {"description": str(message)[:500], "done": done},
                }
            logger.debug(f"å‘é€çŠ¶æ€æ›´æ–°ï¼š{status_payload}")
            await self.emitter(status_payload)

    def get_models(self) -> List[dict]:
        return [
            {
                "id": self.valves.model_id,
                "name": self.valves.model_display_name,
            },
        ]

    def pipes(self) -> List[dict]:
        return self.get_models()

    def split_html_tags(self, text: str) -> List[str]:
        """
        å°†æ–‡æœ¬åˆ†å‰²ä¸º HTML æ ‡ç­¾å’Œæ™®é€šæ–‡æœ¬å—çš„åˆ—è¡¨
        ä¾‹å¦‚ï¼š"Hello <b>world</b>!" -> ["Hello ", "<b>", "world", "</b>", "!"]
        """
        import re

        pattern = r"(<[^>]+>)"
        return re.split(pattern, text)

    async def process_stream(
        self, response: httpx.Response
    ) -> AsyncGenerator[str, None]:
        """
        å¤„ç†æ¥è‡ª Gemini API çš„æœåŠ¡å™¨å‘é€äº‹ä»¶ (SSE) æµï¼Œå¹¶å¢åŠ äº†è¶…æ—¶å’Œå®Œæ•´æ€§æ£€æŸ¥ã€‚
        """
        logger.info("å¼€å§‹å¤„ç† API å“åº”æµã€‚")
        finish_reason_received = False
        stream_iterator = response.aiter_lines()
        content_yielded = False

        try:
            while True:
                try:
                    line = await asyncio.wait_for(
                        stream_iterator.__anext__(),
                        timeout=self.valves.stream_idle_timeout,
                    )

                    if not line.strip() or not line.startswith("data: "):
                        continue

                    line = line[6:]

                    try:
                        chunk = json.loads(line)

                        if "error" in chunk:
                            error_detail = chunk.get("error", {}).get(
                                "message", "æµä¸­å‘ç”ŸæœªçŸ¥é”™è¯¯"
                            )
                            error_msg = f"ğŸš¨ Gemini API é”™è¯¯ï¼š{error_detail}"
                            logger.error(error_msg)
                            await self.emit_status(error_msg, done=True)
                            yield error_msg
                            return

                        if "candidates" in chunk:
                            for candidate in chunk.get("candidates", []):
                                if (
                                    "content" in candidate
                                    and "parts" in candidate["content"]
                                ):
                                    for part in candidate["content"]["parts"]:
                                        is_thought_part = part.get("thought") is True
                                        text_content = part.get("text", "")

                                        if not text_content.strip():
                                            continue

                                        if is_thought_part:
                                            thought_text = text_content.strip()
                                            quoted_lines = [
                                                f"> {line}"
                                                for line in thought_text.splitlines()
                                            ]
                                            quoted_thought = "\n".join(quoted_lines)
                                            thought_msg = (
                                                f"<think>{quoted_thought}</think>"
                                            )
                                            yield thought_msg
                                            content_yielded = True
                                        else:
                                            yield text_content
                                            content_yielded = True

                        # --- Token è®¡ç®—é€»è¾‘ ---
                        if usage_metadata := chunk.get("usageMetadata"):
                            usage_parts = []
                            prompt_tokens = usage_metadata.get("promptTokenCount", 0)
                            candidates_tokens = usage_metadata.get(
                                "candidatesTokenCount", 0
                            )
                            total_tokens = usage_metadata.get("totalTokenCount", 0)

                            thoughts_tokens = usage_metadata.get(
                                "thoughtsTokenCount", 0
                            )
                            tool_use_tokens = usage_metadata.get("toolUseTokenCount", 0)
                            grounding_tokens = usage_metadata.get(
                                "groundingTokenCount", 0
                            )

                            thinking_and_tool_tokens = (
                                thoughts_tokens + tool_use_tokens + grounding_tokens
                            )
                            output_text_tokens = (
                                candidates_tokens - thinking_and_tool_tokens
                            )

                            usage_parts.append(f"è¾“å…¥ï¼š{prompt_tokens}")
                            if output_text_tokens > 0:
                                usage_parts.append(f"è¾“å‡ºï¼š{output_text_tokens}")
                            if thinking_and_tool_tokens > 0:
                                usage_parts.append(
                                    f"æ€è€ƒ/å·¥å…·ï¼š{thinking_and_tool_tokens}"
                                )
                            usage_parts.append(f"æ€»è®¡ï¼š{total_tokens}")

                            usage_msg = (
                                f"Token ç”¨é‡ï¼š{', '.join(usage_parts)}"
                                if usage_parts
                                else "ç”¨é‡ä¿¡æ¯å¯ç”¨"
                            )
                            await self.emit_status(usage_msg, done=False)
                        # --- ç»“æŸ Token è®¡ç®— ---

                        if finish_reason := chunk.get("candidates", [{}])[0].get(
                            "finishReason"
                        ):
                            logger.info(f"API å®ŒæˆåŸå› ï¼š{finish_reason}")
                            finish_reason_received = True

                    except json.JSONDecodeError:
                        logger.warning(f"è§£ç  JSON è¡Œå¤±è´¥ï¼š{line}")
                    except (KeyError, IndexError) as e:
                        logger.debug(f"æ•°æ®å—è§£æé”™è¯¯ï¼š{e}")

                except StopAsyncIteration:
                    break
                except asyncio.TimeoutError:
                    error_msg = (
                        f"ğŸš¨ æµè¶…æ—¶ï¼š{self.valves.stream_idle_timeout}ç§’æ— æ•°æ®ã€‚"
                    )
                    logger.error(error_msg)
                    await self.emit_status(error_msg, done=True)
                    yield error_msg
                    return

        finally:
            if not finish_reason_received and content_yielded:
                logger.warning("æµç»“æŸä½†æœªæ”¶åˆ° finishReasonã€‚")

    async def get_request_stream(
        self, messages: list, model_name: str
    ) -> AsyncGenerator[str, None]:
        """æ„å»ºè¯·æ±‚å¹¶ä» Gemini API æµå¼ä¼ è¾“å“åº”ã€‚"""
        api_model = self.valves.api_model

        gemini_contents = []
        for msg in messages:
            role = "user" if msg.get("role") == "user" else "model"
            content = msg.get("content")
            parts = []
            if isinstance(content, str):
                parts.append({"text": content})
            elif isinstance(content, list):
                for part in content:
                    part_type = part.get("type")
                    if part_type == "text":
                        parts.append({"text": part.get("text", "")})
                    elif part_type == "image_url":
                        image_url = part.get("image_url", {}).get("url", "")
                        if (
                            image_url.startswith("data:image")
                            and ";base64," in image_url
                        ):
                            try:
                                header, encoded_data = image_url.split(",", 1)
                                mime_type = header.split(":", 1)[1].split(";", 1)[0]
                                parts.append(
                                    {
                                        "inlineData": {
                                            "mimeType": mime_type,
                                            "data": encoded_data,
                                        }
                                    }
                                )
                            except Exception:
                                pass
            if parts:
                gemini_contents.append({"role": role, "parts": parts})

        # å¦‚æœæœ€åä¸€æ¡æ˜¯ modelï¼Œè¡¥ä¸€ä¸ª user continue (è¿™æ˜¯ API çš„è¦æ±‚ï¼Œä¸èƒ½ä»¥ model ç»“å°¾)
        if gemini_contents and gemini_contents[-1]["role"] == "model":
            gemini_contents.append({"role": "user", "parts": [{"text": "Continue"}]})

        # å¯ç”¨å·¥å…·ï¼šæœç´¢å’Œä»£ç æ‰§è¡Œ
        gemini_tools = [
            {"googleSearch": {}},
            {"code_execution": {}},
            {"url_context": {}},
        ]

        data = {
            "contents": gemini_contents,
            "tools": gemini_tools,
            "generationConfig": {
                "temperature": self.valves.temperature,
                "topP": self.valves.top_p,
                "thinkingConfig": {
                    "includeThoughts": self.valves.include_thoughts,
                    "thinkingBudget": self.valves.thinking_budget,
                },
            },
        }

        url = f"/v1beta/models/{api_model}:streamGenerateContent?key={self.valves.api_key}&alt=sse"

        try:
            async with httpx.AsyncClient(
                base_url=self.valves.base_url,
                trust_env=True,
                timeout=self.valves.timeout,
            ) as client:
                async with client.stream("POST", url, json=data) as response:
                    if response.status_code != 200:
                        error_content = await response.aread()
                        error_message = f"ğŸš¨ API é”™è¯¯ï¼š{response.status_code} - {error_content.decode()}"
                        yield error_message
                        return

                    async for content in self.process_stream(response):
                        yield content

        except Exception as e:
            error_msg = f"ğŸš¨ è¯·æ±‚å¼‚å¸¸ï¼š{e}"
            logger.exception(error_msg)
            yield error_msg

    async def check_completion(self, messages: list, last_response: str) -> bool:
        """
        ä½¿ç”¨æ¨¡å‹ç‹¬ç«‹åˆ¤æ–­ï¼šå›å¤æ˜¯å¦æ»¡è¶³äº†ç”¨æˆ·çš„è¦æ±‚ï¼ˆè€Œä¸ä»…ä»…æ˜¯å¥å­å®Œæ•´ï¼‰ã€‚
        """
        # æå–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯ä½œä¸ºä¸Šä¸‹æ–‡
        last_user_msg = "N/A"
        for msg in reversed(messages):
            if msg.get("role") == "user":
                content = msg.get("content")
                if isinstance(content, str):
                    last_user_msg = content
                elif isinstance(content, list):
                    # ç®€åŒ–å¤„ç†ï¼Œåªå–æ–‡æœ¬éƒ¨åˆ†
                    texts = [
                        p.get("text", "") for p in content if p.get("type") == "text"
                    ]
                    last_user_msg = " ".join(texts)
                break

        # --- æ ¸å¿ƒä¿®æ”¹ï¼šæ›´ä¸¥æ ¼çš„ä¸šåŠ¡é€»è¾‘å®Œæˆæ€§æ£€æŸ¥ Prompt ---
        judge_prompt = f"""
You are a strict Quality Assurance Validator for an AI assistant.
Your task is to determine if the Model Response **fully satisfies** the User Request.

User Request:
"{last_user_msg[:2000]}"

Model Response (to evaluate):
"{last_response}"

Evaluation Criteria:
1. **Requirement Fulfillment**: Did the model do what was asked? (e.g., if asked for code, is the code there? If asked for a list of 10, are there 10?)
2. **Completeness**: Is the answer cut off in the middle of a sentence, a list, or a code block?
3. **Conclusion**: Does the response have a natural conclusion or closing?

Instructions:
- If the response is cut off, incomplete, or misses part of the user's instruction, reply "INCOMPLETE".
- If the response effectively answers the prompt and is syntactically finished, reply "COMPLETE".

Reply ONLY with the word "COMPLETE" or "INCOMPLETE". Do not explain.
"""

        payload = {
            "contents": [{"role": "user", "parts": [{"text": judge_prompt}]}],
            "generationConfig": {
                "temperature": 0.0,  # ç¡®å®šæ€§è¾“å‡º
                "maxOutputTokens": 5,
            },
        }

        judge_model = self.valves.api_model
        url = f"/v1beta/models/{judge_model}:generateContent?key={self.valves.api_key}"

        try:
            async with httpx.AsyncClient(
                base_url=self.valves.base_url, trust_env=True, timeout=30
            ) as client:
                response = await client.post(url, json=payload)
                if response.status_code == 200:
                    data = response.json()
                    text = (
                        data.get("candidates", [{}])[0]
                        .get("content", {})
                        .get("parts", [{}])[0]
                        .get("text", "")
                        .strip()
                        .upper()
                    )
                    logger.info(f"ğŸ” éœ€æ±‚æ»¡è¶³åº¦/å®Œæ•´æ€§æ£€æŸ¥ç»“æœï¼š{text}")
                    return "COMPLETE" in text
                else:
                    logger.warning(
                        f"å®Œæ•´æ€§æ£€æŸ¥å¤±è´¥ ({response.status_code})ï¼Œé»˜è®¤é€šè¿‡ä»¥é˜²æ­»å¾ªç¯ã€‚"
                    )
                    return True
        except Exception as e:
            logger.error(f"å®Œæ•´æ€§æ£€æŸ¥å‘ç”Ÿå¼‚å¸¸ï¼š{e}ï¼Œé»˜è®¤ä¸º True")
            return True

    async def pipe(
        self,
        body: dict,
        __user__: Optional[dict] = None,
        __event_emitter__: Optional[Callable[[dict], Awaitable[None]]] = None,
        __event_call__: Optional[Callable[[dict], Awaitable[dict]]] = None,
    ) -> AsyncGenerator[str, None]:
        self.emitter = __event_emitter__
        request_id = str(uuid.uuid4())
        logger.info(f"[{request_id}] å¼€å§‹å¤„ç†è¯·æ±‚ã€‚")

        try:
            if not self.valves.api_key:
                yield "âŒ é”™è¯¯ï¼šæœªè®¾ç½® API å¯†é’¥ã€‚"
                return

            messages = body.get("messages", [])
            model_id = body.get("model", self.valves.model_id)

            loop_count = 0
            max_loops = 10  # é€‚å½“é™ä½æœ€å¤§å¾ªç¯æ¬¡æ•°ï¼Œé˜²æ­¢æ— é™çº ç¼ 

            while loop_count < max_loops:
                loop_count += 1
                stream_had_error = False
                full_response_this_turn = ""

                # 1. æ‰§è¡Œæµå¼ç”Ÿæˆ
                async for chunk in self.get_request_stream(messages, model_id):
                    if chunk.startswith("ğŸš¨"):
                        stream_had_error = True
                        yield chunk
                        continue

                    full_response_this_turn += chunk

                    # æ ¹æ®é…ç½®è¾“å‡º
                    if self.valves.block_size > 1:
                        for part in self.split_html_tags(chunk):
                            if part.startswith("<") and part.endswith(">"):
                                yield part
                            else:
                                for i in range(0, len(part), self.valves.block_size):
                                    yield part[i : i + self.valves.block_size]
                                    if self.valves.output_delay > 0:
                                        await asyncio.sleep(self.valves.output_delay)
                    elif self.valves.block_size < 0:
                        for part in self.split_html_tags(chunk):
                            if part.startswith("<") and part.endswith(">"):
                                yield part
                            else:
                                for word in part.split():
                                    yield word + " "
                                    if self.valves.output_delay > 0:
                                        await asyncio.sleep(self.valves.output_delay)
                    else:
                        for char in chunk:
                            yield char
                            if self.valves.output_delay > 0 and char not in ["<", ">"]:
                                await asyncio.sleep(self.valves.output_delay)

                if stream_had_error:
                    break

                if not full_response_this_turn.strip():
                    logger.warning("æ”¶åˆ°ç©ºå“åº”ï¼Œåœæ­¢ç”Ÿæˆã€‚")
                    break

                # 2. ç‹¬ç«‹åˆ¤æ–­æ˜¯å¦ç»“æŸ
                await asyncio.sleep(0.2)
                await self.emit_status(
                    f"æ­£åœ¨éªŒè¯å›ç­”æ˜¯å¦æ»¡è¶³è¦æ±‚... (ç¬¬ {loop_count} è½®)", done=False
                )

                is_complete = await self.check_completion(
                    messages, full_response_this_turn
                )

                if is_complete:
                    logger.info(f"[{request_id}] åˆ¤å®šå›ç­”å·²æ»¡è¶³è¦æ±‚ã€‚")
                    break
                else:
                    logger.info(
                        f"[{request_id}] åˆ¤å®šå›ç­”æœªå®Œæˆ/æœªæ»¡è¶³è¦æ±‚ï¼Œç»§ç»­ç”Ÿæˆ..."
                    )
                    await self.emit_status(
                        f"å›ç­”æœªå®Œæˆæˆ–æœªæ»¡è¶³è¦æ±‚ï¼Œæ­£åœ¨ç»§ç»­... (ç¬¬ {loop_count + 1} è½®)",
                        done=False,
                    )

                    # æ›´æ–°å†å²
                    messages.append(
                        {"role": "model", "content": full_response_this_turn}
                    )
                    # æç¤ºè¯ç¨å¾®ä¿®æ”¹ï¼Œå¼ºè°ƒç»§ç»­å®Œæˆ
                    messages.append(
                        {
                            "role": "user",
                            "content": "It seems the previous response was incomplete or cut off. Please continue exactly from where you left off to fully satisfy the original request.",
                        }
                    )

                    yield "\n"  # è§†è§‰åˆ†éš”

            if loop_count >= max_loops:
                yield "\n\n[è¾¾åˆ°æœ€å¤§è‡ªåŠ¨ç»­å†™æ¬¡æ•°é™åˆ¶]"

            await self.emit_status("ç”Ÿæˆå®Œæˆã€‚", done=True)

        except Exception as e:
            logger.exception(f"[{request_id}] è‡´å‘½é”™è¯¯ï¼š{e}")
            yield f"âŒ ç³»ç»Ÿé”™è¯¯ï¼š{e}"
