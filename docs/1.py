"""
title: Gemini with search & code (Pseudo-streaming) - Robust Version with Independent Judgment
licence: MIT
"""

import json
import logging
import time
import uuid
from typing import Optional, Callable, Awaitable, AsyncGenerator, List
import asyncio

import httpx
from pydantic import BaseModel, Field

# å‡è®¾ open_webui.env å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨ï¼Œåˆ™ä½¿ç”¨æ ‡å‡†çš„ logging é…ç½®
try:
    from open_webui.env import SRC_LOG_LEVELS

    log_level = SRC_LOG_LEVELS["MAIN"]
except ImportError:
    log_level = logging.INFO

# é…ç½®æ—¥å¿—è®°å½•å™¨
logging.basicConfig(
    level=log_level, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)
logger.setLevel(log_level)


class Pipe:
    """
    ä¸€ä¸ªç”¨äºä¸ Gemini API äº¤äº’çš„ Manifold é£æ ¼ç®¡é“ã€‚
    è¯¥ç®¡é“å¤„ç†æµå¼å“åº”ï¼Œå¹¶åœ¨æ¯è½®ç»“æŸåä½¿ç”¨æ¨¡å‹ç‹¬ç«‹åˆ¤æ–­æ˜¯å¦éœ€è¦ç»§ç»­ç”Ÿæˆã€‚
    """

    class Valves(BaseModel):
        # Pydantic æ¨¡å‹ï¼Œç”¨äºé…ç½®ç®¡é“çš„é˜€é—¨ï¼ˆå³è®¾ç½®ï¼‰
        base_url: str = Field(
            default="https://generativelanguage.googleapis.com",
            description="Gemini API çš„åŸºç¡€ URL",
        )
        api_key: str = Field(default="", description="Gemini API å¯†é’¥")
        timeout: int = Field(default=600, description="æ•´ä¸ªè¯·æ±‚çš„è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰")

        # æ–°å¢ï¼šæµç©ºé—²è¶…æ—¶ï¼Œç”¨äºæ£€æµ‹ä¸­æ–­çš„æµ
        stream_idle_timeout: int = Field(
            default=30, description="åœ¨å‡å®šè¿æ¥ä¸­æ–­ä¹‹å‰ï¼Œç­‰å¾…æ–°æ•°æ®çš„æœ€é•¿æ—¶é—´ï¼ˆç§’ï¼‰ã€‚"
        )

        # æ¨¡å‹é…ç½®
        model_id: str = Field(
            default="gemini-2.5-flash-lite",
            description="UI ä¸­ä½¿ç”¨çš„æ¨¡å‹ IDã€‚",
        )
        model_display_name: str = Field(
            default="Gemini 2.5 Flash lite ç ”ç©¶", description="UI ä¸­æ˜¾ç¤ºçš„æ¨¡å‹åç§°ã€‚"
        )
        api_model: str = Field(
            default="gemini-2.5-flash-lite",
            description="ç”¨äº API è°ƒç”¨çš„å®é™… Gemini æ¨¡å‹åç§°ã€‚",
        )
        # æ€è€ƒé¢„ç®—é…ç½®
        thinking_budget: int = Field(
            default=-1,
            description="Gemini API çš„æ€è€ƒé¢„ç®—ï¼ˆthinkingBudgetï¼‰ã€‚è®¾ç½®ä¸º 0 å…³é—­æ€è€ƒï¼Œ-1 å¼€å¯åŠ¨æ€æ€è€ƒã€‚",
        )
        include_thoughts: bool = Field(
            default=True, description="æ˜¯å¦è¿”å› Gemini çš„æ€è€ƒæ‘˜è¦"
        )
        # è¾“å‡ºå»¶è¿Ÿé…ç½®
        output_delay: float = Field(
            default=0.01,
            description="è¾“å‡ºå»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰ã€‚åœ¨å­—ç¬¦æ¨¡å¼ä¸‹ä¸ºæ¯ä¸ªå­—ç¬¦é—´çš„å»¶è¿Ÿï¼Œåœ¨å—æ¨¡å¼ä¸‹ä¸ºæ¯ä¸ªå—é—´çš„å»¶è¿Ÿã€‚è®¾ç½®ä¸º 0 ä»¥ç¦ç”¨ã€‚",
        )
        # å—çŠ¶è¾“å‡ºé…ç½®
        block_size: int = Field(
            default=10,
            description="æ¯ä¸ªè¾“å‡ºå—çš„å­—ç¬¦æ•°ã€‚å½“å¤§äº 1 æ—¶åˆ†å—è¾“å‡ºï¼Œä¸º 1 æ—¶é€å­—ç¬¦è¾“å‡ºï¼Œå°äº 0 æ—¶æŒ‰ç©ºæ ¼åˆ†å—è¾“å‡ºã€‚",
        )
        # temperature å’Œ top_P é…ç½®
        temperature: float = Field(default=0.7, description="æ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„éšæœºæ€§ã€‚")
        top_p: float = Field(default=0.9, description="æ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„å¤šæ ·æ€§ã€‚")

    def __init__(self):
        self.type = "manifold"
        self.name = "Gemini 2.5 Flash lite ç ”ç©¶"
        self.valves = self.Valves()
        self.emitter: Optional[Callable[[dict], Awaitable[None]]] = None
        logger.info(f"ç®¡é“ '{self.name}' å·²åˆå§‹åŒ–ã€‚")

    async def emit_status(self, message: str, done: bool = False):
        if self.emitter:
            if message.strip().startswith("<thinking>") and message.strip().endswith(
                "</thinking>"
            ):
                status_payload = {
                    "type": "status",
                    "data": {"description": message, "done": done},
                }
            else:
                status_payload = {
                    "type": "status",
                    "data": {"description": str(message)[:500], "done": done},
                }
            logger.debug(f"å‘é€çŠ¶æ€æ›´æ–°ï¼š{status_payload}")
            await self.emitter(status_payload)

    def get_models(self) -> List[dict]:
        return [
            {
                "id": self.valves.model_id,
                "name": self.valves.model_display_name,
            },
        ]

    def pipes(self) -> List[dict]:
        return self.get_models()

    def split_html_tags(self, text: str) -> List[str]:
        """
        å°†æ–‡æœ¬åˆ†å‰²ä¸º HTML æ ‡ç­¾å’Œæ™®é€šæ–‡æœ¬å—çš„åˆ—è¡¨
        ä¾‹å¦‚ï¼š"Hello <b>world</b>!" -> ["Hello ", "<b>", "world", "</b>", "!"]
        """
        import re
        pattern = r'(<[^>]+>)'
        return re.split(pattern, text)

    async def process_stream(
        self, response: httpx.Response
    ) -> AsyncGenerator[str, None]:
        """
        å¤„ç†æ¥è‡ª Gemini API çš„æœåŠ¡å™¨å‘é€äº‹ä»¶ (SSE) æµï¼Œå¹¶å¢åŠ äº†è¶…æ—¶å’Œå®Œæ•´æ€§æ£€æŸ¥ã€‚
        """
        logger.info("å¼€å§‹å¤„ç† API å“åº”æµã€‚")
        finish_reason_received = False
        stream_iterator = response.aiter_lines()
        content_yielded = False

        try:
            while True:
                try:
                    line = await asyncio.wait_for(
                        stream_iterator.__anext__(),
                        timeout=self.valves.stream_idle_timeout,
                    )

                    if not line.strip() or not line.startswith("data: "):
                        continue

                    line = line[6:]

                    try:
                        chunk = json.loads(line)
                        # logger.debug(f"æ”¶åˆ°å¹¶è§£æäº†æ•°æ®å—ï¼š{chunk}")

                        if "error" in chunk:
                            error_detail = chunk.get("error", {}).get(
                                "message", "æµä¸­å‘ç”ŸæœªçŸ¥é”™è¯¯"
                            )
                            error_msg = f"ğŸš¨ Gemini API é”™è¯¯ï¼š{error_detail}"
                            logger.error(error_msg)
                            await self.emit_status(error_msg, done=True)
                            yield error_msg
                            return

                        if "candidates" in chunk:
                            for candidate in chunk.get("candidates", []):
                                if (
                                    "content" in candidate
                                    and "parts" in candidate["content"]
                                ):
                                    for part in candidate["content"]["parts"]:
                                        is_thought_part = part.get("thought") is True
                                        text_content = part.get("text", "")

                                        if not text_content.strip():
                                            continue

                                        if is_thought_part:
                                            thought_text = text_content.strip()
                                            quoted_lines = [
                                                f"> {line}"
                                                for line in thought_text.splitlines()
                                            ]
                                            quoted_thought = "\n".join(quoted_lines)
                                            thought_msg = (
                                                f"<think>{quoted_thought}</think>"
                                            )
                                            yield thought_msg
                                            content_yielded = True
                                        else:
                                            yield text_content
                                            content_yielded = True

                        # --- Token è®¡ç®—é€»è¾‘ ---
                        if usage_metadata := chunk.get("usageMetadata"):
                            usage_parts = []
                            prompt_tokens = usage_metadata.get("promptTokenCount", 0)
                            candidates_tokens = usage_metadata.get(
                                "candidatesTokenCount", 0
                            )
                            total_tokens = usage_metadata.get("totalTokenCount", 0)

                            thoughts_tokens = usage_metadata.get(
                                "thoughtsTokenCount", 0
                            )
                            tool_use_tokens = usage_metadata.get("toolUseTokenCount", 0)
                            grounding_tokens = usage_metadata.get(
                                "groundingTokenCount", 0
                            )

                            thinking_and_tool_tokens = (
                                thoughts_tokens + tool_use_tokens + grounding_tokens
                            )
                            output_text_tokens = (
                                candidates_tokens - thinking_and_tool_tokens
                            )

                            usage_parts.append(f"è¾“å…¥ï¼š{prompt_tokens}")
                            if output_text_tokens > 0:
                                usage_parts.append(f"è¾“å‡ºï¼š{output_text_tokens}")
                            if thinking_and_tool_tokens > 0:
                                usage_parts.append(
                                    f"æ€è€ƒ/å·¥å…·ï¼š{thinking_and_tool_tokens}"
                                )
                            usage_parts.append(f"æ€»è®¡ï¼š{total_tokens}")

                            usage_msg = (
                                f"Token ç”¨é‡ï¼š{', '.join(usage_parts)}"
                                if usage_parts
                                else "ç”¨é‡ä¿¡æ¯å¯ç”¨"
                            )
                            # ä¸å†é¢‘ç¹ logï¼Œé¿å…åˆ·å±ï¼Œä»…å‘é€çŠ¶æ€
                            # logger.info(usage_msg) 
                            await self.emit_status(usage_msg, done=False)
                        # --- ç»“æŸ Token è®¡ç®— ---

                        if finish_reason := chunk.get("candidates", [{}])[0].get(
                            "finishReason"
                        ):
                            logger.info(f"API å®ŒæˆåŸå› ï¼š{finish_reason}")
                            finish_reason_received = True

                    except json.JSONDecodeError:
                        logger.warning(f"è§£ç  JSON è¡Œå¤±è´¥ï¼š{line}")
                    except (KeyError, IndexError) as e:
                        logger.debug(f"æ•°æ®å—è§£æé”™è¯¯ï¼š{e}")

                except StopAsyncIteration:
                    break
                except asyncio.TimeoutError:
                    error_msg = f"ğŸš¨ æµè¶…æ—¶ï¼š{self.valves.stream_idle_timeout}ç§’æ— æ•°æ®ã€‚"
                    logger.error(error_msg)
                    await self.emit_status(error_msg, done=True)
                    yield error_msg
                    return

        finally:
            if not finish_reason_received and content_yielded:
                logger.warning("æµç»“æŸä½†æœªæ”¶åˆ° finishReasonã€‚")

    async def get_request_stream(
        self, messages: list, model_name: str
    ) -> AsyncGenerator[str, None]:
        """æ„å»ºè¯·æ±‚å¹¶ä» Gemini API æµå¼ä¼ è¾“å“åº”ã€‚"""
        api_model = self.valves.api_model
        
        gemini_contents = []
        for msg in messages:
            role = "user" if msg.get("role") == "user" else "model"
            content = msg.get("content")
            parts = []
            if isinstance(content, str):
                parts.append({"text": content})
            elif isinstance(content, list):
                for part in content:
                    part_type = part.get("type")
                    if part_type == "text":
                        parts.append({"text": part.get("text", "")})
                    elif part_type == "image_url":
                        image_url = part.get("image_url", {}).get("url", "")
                        if image_url.startswith("data:image") and ";base64," in image_url:
                            try:
                                header, encoded_data = image_url.split(",", 1)
                                mime_type = header.split(":", 1)[1].split(";", 1)[0]
                                parts.append({
                                    "inlineData": {
                                        "mimeType": mime_type,
                                        "data": encoded_data
                                    }
                                })
                            except Exception:
                                pass
            if parts:
                gemini_contents.append({"role": role, "parts": parts})

        # å¦‚æœæœ€åä¸€æ¡æ˜¯ modelï¼Œè¡¥ä¸€ä¸ª user continue (è¿™æ˜¯ API çš„è¦æ±‚ï¼Œä¸èƒ½ä»¥ model ç»“å°¾)
        if gemini_contents and gemini_contents[-1]["role"] == "model":
            gemini_contents.append({"role": "user", "parts": [{"text": "Continue"}]})

        gemini_tools = [{"googleSearch": {}}, {"code_execution": {}}]

        data = {
            "contents": gemini_contents,
            "tools": gemini_tools,
            "generationConfig": {
                "temperature": self.valves.temperature,
                "topP": self.valves.top_p,
                "thinkingConfig": {
                    "includeThoughts": self.valves.include_thoughts,
                    "thinkingBudget": self.valves.thinking_budget,
                },
            },
        }

        url = f"/v1beta/models/{api_model}:streamGenerateContent?key={self.valves.api_key}&alt=sse"

        try:
            async with httpx.AsyncClient(
                base_url=self.valves.base_url,
                trust_env=True,
                timeout=self.valves.timeout,
            ) as client:
                async with client.stream("POST", url, json=data) as response:
                    if response.status_code != 200:
                        error_content = await response.aread()
                        error_message = f"ğŸš¨ API é”™è¯¯ï¼š{response.status_code} - {error_content.decode()}"
                        yield error_message
                        return

                    async for content in self.process_stream(response):
                        yield content

        except Exception as e:
            error_msg = f"ğŸš¨ è¯·æ±‚å¼‚å¸¸ï¼š{e}"
            logger.exception(error_msg)
            yield error_msg

    async def check_completion(self, messages: list, last_response: str) -> bool:
        """
        ä½¿ç”¨æ¨¡å‹ç‹¬ç«‹åˆ¤æ–­æœ€åçš„å›å¤æ˜¯å¦å®Œæ•´ã€‚
        """
        # æå–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯ä½œä¸ºä¸Šä¸‹æ–‡
        last_user_msg = "N/A"
        for msg in reversed(messages):
            if msg.get("role") == "user":
                content = msg.get("content")
                if isinstance(content, str):
                    last_user_msg = content
                elif isinstance(content, list):
                    # ç®€åŒ–å¤„ç†ï¼Œåªå–æ–‡æœ¬éƒ¨åˆ†
                    texts = [p.get("text", "") for p in content if p.get("type") == "text"]
                    last_user_msg = " ".join(texts)
                break
        
        # æ„é€ åˆ¤æ–­æç¤ºè¯
        judge_prompt = f"""
You are a strict output validator.
Your task is to analyze the following response from an AI model corresponding to the user's request and determine if the response is COMPLETE or INCOMPLETE.

User Request (Context):
"{last_user_msg[:2000]}" ... (truncated)

Model Response to Evaluate:
"{last_response}"

Criteria:
1. Does the response finish its sentence?
2. Does it seem to abruptly stop?
3. Does it fully address the request or is it clearly cut off?

Reply ONLY with the word "COMPLETE" if it is finished, or "INCOMPLETE" if it needs to continue. Do not output anything else.
"""
        
        payload = {
            "contents": [{"role": "user", "parts": [{"text": judge_prompt}]}],
            "generationConfig": {
                "temperature": 0.0, # ç¡®å®šæ€§è¾“å‡º
                "maxOutputTokens": 10
            }
        }
        
        # ä½¿ç”¨ fast/lite æ¨¡å‹è¿›è¡Œåˆ¤æ–­ï¼Œæˆ–è€…å¤ç”¨å½“å‰æ¨¡å‹
        judge_model = self.valves.api_model 
        url = f"/v1beta/models/{judge_model}:generateContent?key={self.valves.api_key}"

        try:
            async with httpx.AsyncClient(base_url=self.valves.base_url, trust_env=True, timeout=30) as client:
                response = await client.post(url, json=payload)
                if response.status_code == 200:
                    data = response.json()
                    text = data.get("candidates", [{}])[0].get("content", {}).get("parts", [{}])[0].get("text", "").strip().upper()
                    logger.info(f"ğŸ” å®Œæ•´æ€§æ£€æŸ¥ç»“æœï¼š{text}")
                    return "COMPLETE" in text
                else:
                    logger.warning(f"å®Œæ•´æ€§æ£€æŸ¥å¤±è´¥ ({response.status_code})ï¼Œé»˜è®¤è®¤ä¸ºå·²å®Œæˆä»¥é˜²æ­¢æ­»å¾ªç¯ã€‚")
                    return True
        except Exception as e:
            logger.error(f"å®Œæ•´æ€§æ£€æŸ¥å‘ç”Ÿå¼‚å¸¸ï¼š{e}ï¼Œé»˜è®¤ä¸º True")
            return True

    async def pipe(
        self,
        body: dict,
        __user__: Optional[dict] = None,
        __event_emitter__: Optional[Callable[[dict], Awaitable[None]]] = None,
        __event_call__: Optional[Callable[[dict], Awaitable[dict]]] = None,
    ) -> AsyncGenerator[str, None]:
        self.emitter = __event_emitter__
        request_id = str(uuid.uuid4())
        logger.info(f"[{request_id}] å¼€å§‹å¤„ç†è¯·æ±‚ã€‚")

        try:
            if not self.valves.api_key:
                yield "âŒ é”™è¯¯ï¼šæœªè®¾ç½® API å¯†é’¥ã€‚"
                return

            messages = body.get("messages", [])
            model_id = body.get("model", self.valves.model_id)

            loop_count = 0
            max_loops = 15 # å®‰å…¨ä¸Šé™
            
            while loop_count < max_loops:
                loop_count += 1
                stream_had_error = False
                full_response_this_turn = ""
                
                # 1. æ‰§è¡Œæµå¼ç”Ÿæˆ
                async for chunk in self.get_request_stream(messages, model_id):
                    if chunk.startswith("ğŸš¨"):
                        stream_had_error = True
                        yield chunk
                        continue

                    full_response_this_turn += chunk

                    # æ ¹æ®é…ç½®è¾“å‡º (å—çŠ¶/å­—ç¬¦/ç©ºæ ¼)
                    if self.valves.block_size > 1:
                        for part in self.split_html_tags(chunk):
                            if part.startswith('<') and part.endswith('>'):
                                yield part
                            else:
                                for i in range(0, len(part), self.valves.block_size):
                                    yield part[i:i + self.valves.block_size]
                                    if self.valves.output_delay > 0: await asyncio.sleep(self.valves.output_delay)
                    elif self.valves.block_size < 0:
                        for part in self.split_html_tags(chunk):
                            if part.startswith('<') and part.endswith('>'):
                                yield part
                            else:
                                for word in part.split():
                                    yield word + ' '
                                    if self.valves.output_delay > 0: await asyncio.sleep(self.valves.output_delay)
                    else:
                        for char in chunk:
                            yield char
                            if self.valves.output_delay > 0 and char not in ['<', '>']:
                                await asyncio.sleep(self.valves.output_delay)

                if stream_had_error:
                    break
                
                if not full_response_this_turn.strip():
                    logger.warning("æ”¶åˆ°ç©ºå“åº”ï¼Œåœæ­¢ç”Ÿæˆã€‚")
                    break

                # 2. ç‹¬ç«‹åˆ¤æ–­æ˜¯å¦ç»“æŸ
                # ç­‰å¾…æµç¨å¾®ç¨³å®š
                await asyncio.sleep(0.2)
                await self.emit_status(f"æ­£åœ¨éªŒè¯å›ç­”å®Œæ•´æ€§... (ç¬¬ {loop_count} è½®)", done=False)
                
                is_complete = await self.check_completion(messages, full_response_this_turn)

                if is_complete:
                    logger.info(f"[{request_id}] åˆ¤å®šå›ç­”å·²å®Œæ•´ã€‚")
                    break
                else:
                    logger.info(f"[{request_id}] åˆ¤å®šå›ç­”ä¸å®Œæ•´ï¼Œå‡†å¤‡ç»§ç»­...")
                    await self.emit_status(f"æ£€æµ‹åˆ°å›ç­”æœªå®Œæˆï¼Œè‡ªåŠ¨ç»§ç»­ç”Ÿæˆ... (ç¬¬ {loop_count + 1} è½®)", done=False)
                    
                    # æ›´æ–°å†å²
                    messages.append({"role": "model", "content": full_response_this_turn})
                    messages.append({"role": "user", "content": "Please continue strictly from where you stopped."})
                    
                    # å‘é€æ¢è¡Œç¬¦ä»¥åˆ†éš”æ¥ä¸‹æ¥çš„å†…å®¹
                    yield "\n" 

            if loop_count >= max_loops:
                yield "\n\n[è¾¾åˆ°æœ€å¤§è‡ªåŠ¨ç»­å†™æ¬¡æ•°é™åˆ¶]"

            await self.emit_status("ç”Ÿæˆå®Œæˆã€‚", done=True)

        except Exception as e:
            logger.exception(f"[{request_id}] è‡´å‘½é”™è¯¯ï¼š{e}")
            yield f"âŒ ç³»ç»Ÿé”™è¯¯ï¼š{e}"