"""
title: Gemini 5-Step Agent (Clean Output)
licence: MIT
author: OpenWebUI User
description: Forces Gemini to follow a strict 5-step reasoning loop. Fixed async generator return error and removed redundant headers.
"""

import json
import logging
import re
import asyncio
from typing import Optional, Callable, Awaitable, AsyncGenerator, List
import httpx
from pydantic import BaseModel, Field

# --- æ—¥å¿—é…ç½® ---
try:
    from open_webui.env import SRC_LOG_LEVELS

    log_level = SRC_LOG_LEVELS["MAIN"]
except ImportError:
    log_level = logging.INFO

logging.basicConfig(
    level=log_level, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)
logger.setLevel(log_level)


class Pipe:
    """
    Gemini 5-Step Agent Pipe - V3.3 Clean
    æµç¨‹ï¼šPlan -> Act 1 -> Review -> Act 2 -> Summarize
    ä¼˜åŒ–ï¼šåˆ é™¤äº†èŠå¤©æ°”æ³¡ä¸­çš„é˜¶æ®µæ ‡é¢˜æ–‡æœ¬ï¼Œåªä¿ç•™çŠ¶æ€æ æ˜¾ç¤ºã€‚
    """

    class Valves(BaseModel):
        base_url: str = Field(
            default="https://generativelanguage.googleapis.com",
            description="Gemini API çš„åŸºç¡€ URL",
        )
        api_key: str = Field(default="", description="Gemini API å¯†é’¥")
        timeout: int = Field(default=600, description="æ•´ä¸ªè¯·æ±‚çš„è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰")

        # æ¨¡å‹é…ç½®
        model_id: str = Field(
            default="gemini-2.5-flash-lite",
            description="UI ä¸­ä½¿ç”¨çš„æ¨¡å‹ IDã€‚",
        )
        model_display_name: str = Field(
            default="Gemini 2.5 Flash lite ç ”ç©¶", description="UI ä¸­æ˜¾ç¤ºçš„æ¨¡å‹åç§°ã€‚"
        )
        api_model: str = Field(
            default="gemini-2.5-flash-lite",
            description="ç”¨äº API è°ƒç”¨çš„å®é™… Gemini æ¨¡å‹åç§°ã€‚",
        )
        # æ€è€ƒé¢„ç®—é…ç½®
        thinking_budget: int = Field(
            default=-1,
            description="Gemini API çš„æ€è€ƒé¢„ç®—ï¼ˆthinkingBudgetï¼‰ã€‚è®¾ç½®ä¸º 0 å…³é—­æ€è€ƒï¼Œ-1 å¼€å¯åŠ¨æ€æ€è€ƒã€‚",
        )
        include_thoughts: bool = Field(
            default=False,
            description="æ˜¯å¦æ˜¾ç¤ºåŸç”Ÿ API çš„æ€è€ƒæ ‡ç­¾ï¼ˆå»ºè®® Falseï¼Œä»¥å…æ··æ·†ï¼‰ã€‚",
        )
        output_delay: float = Field(
            default=0.01,
            description="æ‰“å­—æœºæ•ˆæœå»¶è¿Ÿï¼ˆç§’ï¼‰ã€‚",
        )

        # ç»­å†™/Token é…ç½®
        max_step_continuation: int = Field(
            default=3,
            description="æ¯ä¸ªæ­¥éª¤ï¼ˆå¦‚ Act 1ï¼‰å†…éƒ¨æœ€å¤§å…è®¸çš„è‡ªåŠ¨ç»­å†™æ¬¡æ•°ï¼ˆé˜²æ­¢å•æ­¥æˆªæ–­ï¼‰ã€‚",
        )
        max_output_tokens: int = Field(
            default=8192, description="å•æ¬¡è¯·æ±‚æœ€å¤§ Token æ•°"
        )
        temperature: float = Field(default=0.7)
        top_p: float = Field(default=0.9)

    def __init__(self):
        self.type = "manifold"
        self.name = "Gemini 5-Step Agent Clean"
        self.valves = self.Valves()
        self.emitter: Optional[Callable[[dict], Awaitable[None]]] = None
        self.last_finish_reason = None

    async def emit_status(self, message: str, done: bool = False):
        """å‘é€çŠ¶æ€æ›´æ–°ç»™ UI"""
        if self.emitter:
            clean_msg = message[:300] + "..." if len(message) > 300 else message
            await self.emitter(
                {
                    "type": "status",
                    "data": {"description": clean_msg, "done": done},
                }
            )

    def get_models(self) -> List[dict]:
        return [{"id": self.valves.model_id, "name": self.valves.model_display_name}]

    def pipes(self) -> List[dict]:
        return self.get_models()

    async def process_stream(
        self, response: httpx.Response
    ) -> AsyncGenerator[str, None]:
        """å¤„ç† SSE æµ"""
        self.last_finish_reason = None

        async for line in response.aiter_lines():
            if not line.strip() or not line.startswith("data: "):
                continue
            line = line[6:]
            if line == "[DONE]":
                break
            try:
                chunk = json.loads(line)
                if "error" in chunk:
                    err_msg = chunk.get("error", {}).get("message", "Unknown Error")
                    yield f"ğŸš¨ API Error: {err_msg}"
                    return

                if "candidates" in chunk:
                    candidate = chunk["candidates"][0]
                    if "finishReason" in candidate:
                        self.last_finish_reason = candidate["finishReason"]

                    if "content" in candidate and "parts" in candidate["content"]:
                        for part in candidate["content"]["parts"]:
                            text_content = part.get("text", "")
                            is_thought = part.get("thought", False)

                            if text_content:
                                if is_thought and self.valves.include_thoughts:
                                    yield f"\n<think>{text_content}</think>\n"
                                elif not is_thought:
                                    yield text_content

            except Exception as e:
                logger.error(f"Stream processing error: {e}")

    async def get_request_stream(self, messages: list) -> AsyncGenerator[str, None]:
        """æ„é€ è¯·æ±‚å¹¶å‘èµ·æµå¼è°ƒç”¨"""

        gemini_contents = []
        for msg in messages:
            role = "model" if msg["role"] == "assistant" else "user"
            parts = [{"text": str(msg.get("content", ""))}]
            if parts:
                gemini_contents.append({"role": role, "parts": parts})

        if gemini_contents and gemini_contents[-1]["role"] == "model":
            gemini_contents.append(
                {"role": "user", "parts": [{"text": "Please continue."}]}
            )

        payload = {
            "contents": gemini_contents,
            "tools": [
                {"googleSearch": {}},
                {"code_execution": {}},
                {"url_context": {}},
            ],
            "generationConfig": {
                "temperature": self.valves.temperature,
                "topP": self.valves.top_p,
                "maxOutputTokens": self.valves.max_output_tokens,
            },
        }

        if self.valves.thinking_budget != 0:
            payload["generationConfig"]["thinkingConfig"] = {
                "includeThoughts": self.valves.include_thoughts,
                "thinkingBudget": self.valves.thinking_budget,
            }

        url = f"/v1beta/models/{self.valves.api_model}:streamGenerateContent?key={self.valves.api_key}&alt=sse"

        try:
            async with httpx.AsyncClient(
                base_url=self.valves.base_url, timeout=self.valves.timeout
            ) as client:
                async with client.stream("POST", url, json=payload) as response:
                    if response.status_code != 200:
                        err = await response.aread()
                        yield f"ğŸš¨ HTTP {response.status_code}: {err.decode()}"
                        return
                    async for chunk in self.process_stream(response):
                        yield chunk
        except Exception as e:
            yield f"ğŸš¨ Connection Error: {e}"

    # --- æ ¸å¿ƒï¼šå•æ­¥æ‰§è¡Œé€»è¾‘ ---
    async def execute_step(
        self, step_name: str, step_prompt: str, context_messages: list
    ) -> AsyncGenerator[str, None]:
        """
        æ‰§è¡Œ 5 æ­¥ä¸­çš„æŸä¸€æ­¥ã€‚
        """

        current_messages = context_messages.copy()

        current_messages.append(
            {
                "role": "user",
                "content": f"**Current Task: {step_name}**\nInstructions: {step_prompt}\n\nProvide your output for this step now.",
            }
        )

        loop_count = 0

        # ã€ä¿®æ”¹ç‚¹ 1ã€‘ç§»é™¤äº†æ­¤å¤„çš„ yield f"\n\n### ğŸŸ¢ {step_name}\n"
        # è¿™æ ·é˜¶æ®µåç§°åªä¼šæ˜¾ç¤ºåœ¨ UI çš„çŠ¶æ€æ ä¸­ï¼Œè€Œä¸ä¼šæ‰“å°åœ¨èŠå¤©æ–‡æœ¬é‡Œ

        while loop_count < self.valves.max_step_continuation:
            loop_count += 1
            chunk_buffer = ""

            async for chunk in self.get_request_stream(current_messages):
                chunk_buffer += chunk
                # æµå¼è¾“å‡ºç»™ç”¨æˆ·
                if self.valves.output_delay > 0:
                    for char in chunk:
                        yield char
                        await asyncio.sleep(self.valves.output_delay / 2)
                else:
                    yield chunk

            # æ£€æŸ¥æ˜¯å¦éœ€è¦ç»­å†™
            if self.last_finish_reason == "MAX_TOKENS":
                await self.emit_status(
                    f"{step_name}: Continuing truncated output...", done=False
                )
                current_messages.append({"role": "model", "content": chunk_buffer})
                current_messages.append({"role": "user", "content": "continue"})
            else:
                break

    async def pipe(
        self,
        body: dict,
        __user__: Optional[dict] = None,
        __event_emitter__: Optional[Callable[[dict], Awaitable[None]]] = None,
    ) -> AsyncGenerator[str, None]:

        self.emitter = __event_emitter__
        messages = body.get("messages", [])

        if not self.valves.api_key:
            yield "Error: API Key not configured."
            return

        steps = [
            (
                "Phase 1: PLAN",
                "Analyze the user's request. Identify the core intent and potential challenges. Create a numbered plan.",
            ),
            (
                "Phase 2: ACT (Initial)",
                "Execute the first steps of your plan. Use tools (Search/Code) if needed. Provide initial results.",
            ),
            (
                "Phase 3: REVIEW",
                "Critically analyze your output from Phase 2. Check for errors, bugs, or missing info.",
            ),
            (
                "Phase 4: ACT (Refinement)",
                "Execute corrections based on the review. Complete the plan.",
            ),
            (
                "Phase 5: SUMMARIZE",
                "Provide the final, polished answer to the user. Synthesize all information. This is the final output.",
            ),
        ]

        internal_history = [msg for msg in messages]

        try:
            total_steps = len(steps)
            for i, (name, prompt) in enumerate(steps):

                await self.emit_status(
                    f"Running {name} ({i+1}/{total_steps})...", done=False
                )

                step_output_buffer = ""  # ç”¨äºæ”¶é›†è¿™ä¸€æ­¥çš„å®Œæ•´æ–‡æœ¬

                # æ‰§è¡Œè¯¥æ­¥éª¤
                async for chunk in self.execute_step(name, prompt, internal_history):
                    if isinstance(chunk, str):
                        yield chunk
                        step_output_buffer += chunk

                # ã€ä¿®æ”¹ç‚¹ 2ã€‘ä¼˜åŒ–å†å²è®°å½•
                # å› ä¸ºæˆ‘ä»¬æ²¡æœ‰åœ¨è¾“å‡ºæµä¸­æ‰“å°æ ‡é¢˜ï¼Œä¸ºäº†è®©æ¨¡å‹è®°ä½å®ƒåˆšåšäº†ä»€ä¹ˆï¼Œ
                # æˆ‘ä»¬åœ¨å­˜å…¥å†å²è®°å½•æ—¶ï¼Œæ˜¾å¼åŠ ä¸Šæ ‡é¢˜ï¼ˆç”¨æˆ·çœ‹ä¸è§ï¼Œä½†æ¨¡å‹çœ‹å¾—è§ï¼‰ã€‚
                clean_content = step_output_buffer.strip()

                internal_history.append(
                    {"role": "user", "content": f"Instruction for {name}: {prompt}"}
                )
                internal_history.append(
                    {"role": "model", "content": f"**{name} Output:**\n{clean_content}"}
                )

                # æ­¥éª¤é—´æ·»åŠ ç®€å•çš„æ¢è¡Œï¼Œé˜²æ­¢æ–‡å­—æŒ¤åœ¨ä¸€èµ·
                yield "\n\n"

            await self.emit_status("All steps completed.", done=True)

        except Exception as e:
            logger.exception(f"Agent Error: {e}")
            yield f"\n\nSystem Error: {str(e)}"
