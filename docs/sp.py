"""
title: Gemini with search & code & OpenWebUI Tools (Pseudo-streaming)
licence: MIT
author: stevessr
author_url: https://linux.do/t/topic/759930
funding_url: https://linux.do/t/topic/759930
version: 0.3
"""

import json
import logging
import time
import uuid
import re
from typing import (
    AsyncIterable,
    Optional,
    Callable,
    Awaitable,
    AsyncGenerator,
    List,
    Dict,
    Any,
)
import asyncio

import httpx
from pydantic import BaseModel, Field

# å‡è®¾ open_webui.env å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨ï¼Œåˆ™ä½¿ç”¨æ ‡å‡†çš„ logging é…ç½®
try:
    from open_webui.env import SRC_LOG_LEVELS

    log_level = SRC_LOG_LEVELS["MAIN"]
except ImportError:
    log_level = logging.INFO

# é…ç½®æ—¥å¿—è®°å½•å™¨
logging.basicConfig(
    level=log_level, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)
logger.setLevel(log_level)


class Pipe:
    """
    ä¸€ä¸ªç”¨äºä¸ Gemini API äº¤äº’çš„ Manifold é£æ ¼ç®¡é“ã€‚
    """

    class Valves(BaseModel):
        # Pydantic æ¨¡å‹ï¼Œç”¨äºé…ç½®ç®¡é“çš„é˜€é—¨ï¼ˆå³è®¾ç½®ï¼‰
        base_url: str = Field(
            default="https://generativelanguage.googleapis.com",
            description="Gemini API çš„åŸºç¡€ URL",
        )
        api_key: str = Field(default="", description="Gemini API å¯†é’¥")
        timeout: int = Field(default=600, description="æ•´ä¸ªè¯·æ±‚çš„è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰")

        # æ–°å¢ï¼šæµç©ºé—²è¶…æ—¶ï¼Œç”¨äºæ£€æµ‹ä¸­æ–­çš„æµ
        stream_idle_timeout: int = Field(
            default=30, description="æµç©ºé—²è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ã€‚"
        )

        # æ¨¡å‹é…ç½®
        model_id: str = Field(
            default="gemini-2.5-flash-lite",
            description="UI ä¸­ä½¿ç”¨çš„æ¨¡å‹ IDã€‚",
        )
        model_display_name: str = Field(
            default="Gemini 2.5 Flash Lite ç ”ç©¶", description="UI ä¸­æ˜¾ç¤ºçš„æ¨¡å‹åç§°ã€‚"
        )
        api_model: str = Field(
            default="gemini-2.5-flash-lite",
            description="ç”¨äº API è°ƒç”¨çš„å®é™… Gemini æ¨¡å‹åç§°ã€‚",
        )

    class UserValves(BaseModel):
        # æ€è€ƒé¢„ç®—é…ç½®
        thinking_budget: int = Field(
            default=-1,
            description="Gemini API çš„æ€è€ƒé¢„ç®—ï¼ˆthinkingBudgetï¼‰ã€‚è®¾ç½®ä¸º 0 å…³é—­æ€è€ƒï¼Œ-1 å¼€å¯åŠ¨æ€æ€è€ƒã€‚",
        )
        include_thoughts: bool = Field(
            default=True, description="æ˜¯å¦è¿”å› Gemini çš„æ€è€ƒæ‘˜è¦"
        )
        # è¾“å‡ºå»¶è¿Ÿé…ç½®
        output_delay: float = Field(
            default=0.01,
            description="è¾“å‡ºå»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰ã€‚åœ¨å­—ç¬¦æ¨¡å¼ä¸‹ä¸ºæ¯ä¸ªå­—ç¬¦é—´çš„å»¶è¿Ÿï¼Œåœ¨å—æ¨¡å¼ä¸‹ä¸ºæ¯ä¸ªå—é—´çš„å»¶è¿Ÿã€‚è®¾ç½®ä¸º 0 ä»¥ç¦ç”¨ã€‚",
        )
        # å—çŠ¶è¾“å‡ºé…ç½®
        block_size: int = Field(
            default=10,
            description="æ¯ä¸ªè¾“å‡ºå—çš„å­—ç¬¦æ•°ã€‚å½“å¤§äº 1 æ—¶åˆ†å—è¾“å‡ºï¼Œä¸º 1 æ—¶é€å­—ç¬¦è¾“å‡ºï¼Œå°äº 0 æ—¶æŒ‰ç©ºæ ¼åˆ†å—è¾“å‡ºã€‚",
        )
        # temperature å’Œ top_P é…ç½®
        temperature: float = Field(default=0.7, description="æ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„éšæœºæ€§ã€‚")
        top_p: float = Field(default=0.9, description="æ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„å¤šæ ·æ€§ã€‚")

    def __init__(self):
        self.type = "manifold"
        self.name = ""
        self.valves = self.Valves()
        self.emitter: Optional[Callable[[dict], Awaitable[None]]] = None
        self.default: Optional[dict] = {
            "thinking_budget": -1,
            "include_thoughts": True,
            "output_delay": 0.01,
            "block_size": 10,
            "temperature": 0.7,
            "top_p": 0.9,
        }
        logger.info(f"ç®¡é“ '{self.name}' å·²åˆå§‹åŒ–ã€‚")

    async def emit_status(self, message: str, done: bool = False):
        if self.emitter:
            await self.emitter(
                {
                    "type": "status",
                    "data": {"description": str(message)[:500], "done": done},
                }
            )

    def get_models(self) -> List[dict]:
        return [
            {"id": self.valves.model_id, "name": self.valves.model_display_name},
        ]

    def pipes(self) -> List[dict]:
        return self.get_models()

    def split_html_tags(self, text: str) -> List[str]:
        """
        å°†æ–‡æœ¬åˆ†å‰²ä¸º HTML æ ‡ç­¾å’Œæ™®é€šæ–‡æœ¬å—çš„åˆ—è¡¨
        ä¾‹å¦‚ï¼š"Hello <b>world</b>!" -> ["Hello ", "<b>", "world", "</b>", "!"]
        """
        pattern = r"(<[^>]+>)"
        return re.split(pattern, text)

    def convert_openai_tools_to_gemini(self, tools: List[Dict]) -> List[Dict]:
        """
        ä¿®å¤ç‰ˆï¼šå°† OpenAI å·¥å…·è½¬æ¢ä¸º Gemini æ ¼å¼ã€‚
        """
        gemini_funcs = []
        for tool in tools:
            if tool.get("type") == "function":
                func = tool.get("function", {})
                name = func.get("name")
                description = func.get("description", "")
                parameters = func.get("parameters", {})

                # Gemini ä¸æ”¯æŒ additionalProperties: false åœ¨æŸäº›å±‚çº§ï¼Œ
                # ä½†é€šå¸¸ OpenAI æ ¼å¼å¯ä»¥ç›´æ¥ä¼ é€’ï¼Œä¸»è¦åŒºåˆ«åœ¨äº properties çš„ç»“æ„ã€‚
                # ç¡®ä¿ parameters æ˜¯ä¸€ä¸ªå¯¹è±¡
                if (
                    parameters.get("type") != "object"
                    and "properties" not in parameters
                ):
                    parameters = {"type": "object", "properties": {}}

                gemini_func = {
                    "name": name,
                    "description": description,
                    "parameters": parameters,
                }
                gemini_funcs.append(gemini_func)
        return gemini_funcs

    def insert_grounding_citations(self, text: str, metadata: Dict) -> str:
        """
        æ ¹æ® groundingMetadata å°†å¼•ç”¨å†…åµŒåˆ°æ–‡æœ¬ä¸­ã€‚
        """
        if not metadata or "groundingChunks" not in metadata or "groundingSupports" not in metadata:
            return text

        chunks = metadata["groundingChunks"]
        supports = metadata["groundingSupports"]

        # æ”¶é›†æ‰€æœ‰çš„æ’å…¥ç‚¹
        # æ ¼å¼ï¼š(index, citation_text)
        insertions = []

        for support in supports:
            segment = support.get("segment", {})
            end_index = segment.get("endIndex")
            chunk_indices = support.get("groundingChunkIndices", [])

            if end_index is not None and chunk_indices:
                # æ„å»ºå¼•ç”¨å­—ç¬¦ä¸²ï¼Œä¾‹å¦‚ [1](url), [2](url)
                citations = []
                for idx in chunk_indices:
                    if 0 <= idx < len(chunks):
                        chunk = chunks[idx]
                        if "web" in chunk:
                            uri = chunk["web"].get("uri", "")
                            # title = chunk["web"].get("title", f"Source {idx+1}")
                            # ä½¿ç”¨ Markdown é“¾æ¥æ ¼å¼ [n](uri)
                            if uri:
                                citations.append(f"[{idx + 1}]({uri})")
                
                if citations:
                    citation_str = " " + ", ".join(citations)
                    insertions.append((end_index, citation_str))

        # æŒ‰ä½ç½®é™åºæ’åºï¼Œä»¥ä¾¿ä»åå¾€å‰æ’å…¥ï¼Œä¸å½±å“å‰é¢çš„ç´¢å¼•
        insertions.sort(key=lambda x: x[0], reverse=True)

        # æ‰§è¡Œæ’å…¥
        result_text = text
        for idx, citation_str in insertions:
            # ç¡®ä¿ç´¢å¼•åœ¨èŒƒå›´å†… (API åº”è¯¥ä¿è¯ï¼Œä½†ä¸ºäº†å®‰å…¨)
            if 0 <= idx <= len(result_text):
                result_text = result_text[:idx] + citation_str + result_text[idx:]
        
        return result_text

    async def process_stream(
        self,
        response: httpx.Response,
        detected_tool_calls: List[Dict],
        accumulated_text: List[str],
    ) -> AsyncGenerator[str, None]:
        """
        å¤„ç†æ¥è‡ª Gemini API çš„ SSE æµã€‚
        æ³¨æ„ï¼šä¸ºäº†æ”¯æŒå†…åµŒå¼•ç”¨ (Inline Grounding)ï¼Œæ™®é€šæ–‡æœ¬ä¼šè¢«ç¼“å†²ç›´åˆ°æµç»“æŸæˆ–æ”¶åˆ°å…ƒæ•°æ®ï¼Œ
        è€Œæ€è€ƒè¿‡ç¨‹ (Thought) ä¼šå®æ—¶æµå¼ä¼ è¾“ã€‚
        """
        finish_reason_received = False
        stream_iterator = response.aiter_lines()
        content_yielded = False
        is_thinking = False
        
        # ç¼“å†²åŒº
        text_buffer = []
        grounding_metadata = None

        try:
            while True:
                try:
                    line = await asyncio.wait_for(
                        stream_iterator.__anext__(),
                        timeout=self.valves.stream_idle_timeout,
                    )
                    if not line.strip() or not line.startswith("data: "):
                        continue
                    line = line[6:]

                    try:
                        chunk = json.loads(line)
                        if "error" in chunk:
                            error_msg = f"ğŸš¨ API Error: {chunk['error'].get('message')}"
                            logger.error(error_msg)
                            yield error_msg
                            return

                        candidates = chunk.get("candidates", [])
                        if candidates:
                            candidate = candidates[0]

                            # 1. æ•è· Grounding Metadata (é€šå¸¸åœ¨æœ€åä¸€ä¸ª chunk)
                            if "groundingMetadata" in candidate:
                                grounding_metadata = candidate["groundingMetadata"]

                            if (
                                "content" in candidate
                                and "parts" in candidate["content"]
                            ):
                                for part in candidate["content"]["parts"]:
                                    # 2. æ£€æµ‹å·¥å…·è°ƒç”¨
                                    if "functionCall" in part:
                                        logger.info(
                                            f"æ£€æµ‹åˆ°å‡½æ•°è°ƒç”¨ï¼š{part['functionCall']}"
                                        )
                                        detected_tool_calls.append(part["functionCall"])
                                        continue  # å·¥å…·è°ƒç”¨ä¸æ˜¾ç¤ºæ–‡æœ¬

                                    # 3. å¤„ç†æ–‡æœ¬å’Œæ€è€ƒ
                                    text_content = part.get("text", "")
                                    is_thought = part.get("thought", False)

                                    if text_content:
                                        content_yielded = True

                                        if is_thought:
                                            # å¦‚æœæ˜¯æ€è€ƒï¼Œç›´æ¥æµå¼è¾“å‡º
                                            if not is_thinking:
                                                yield "<think>"
                                                is_thinking = True
                                            yield text_content
                                        else:
                                            # å¦‚æœæ˜¯æ­£æ–‡ï¼Œå…ˆç»“æŸæ€è€ƒæ ‡ç­¾ï¼ˆå¦‚æœæœ‰å…³é—­çš„è¯ï¼‰
                                            if is_thinking:
                                                yield "</think>"
                                                is_thinking = False
                                            
                                            # ç´¯ç§¯æ­£æ–‡åˆ° history ç”¨äºä¸‹ä¸€æ¬¡ context
                                            accumulated_text.append(text_content)
                                            # ç¼“å†²æ­£æ–‡ç”¨äºæœ€åå¤„ç†å¼•ç”¨
                                            text_buffer.append(text_content)

                        if candidates and candidates[0].get("finishReason"):
                            finish_reason_received = True

                    except json.JSONDecodeError:
                        pass
                    except (KeyError, IndexError):
                        pass

                except StopAsyncIteration:
                    break
                except asyncio.TimeoutError:
                    yield "ğŸš¨ Stream Timeout"
                    return

            if is_thinking:
                yield "</think>"

        finally:
            # æµç»“æŸï¼Œå¤„ç†ç¼“å†²çš„æ–‡æœ¬å’Œå¼•ç”¨
            full_text = "".join(text_buffer)
            
            if grounding_metadata and full_text:
                # å¦‚æœæœ‰å¼•ç”¨å…ƒæ•°æ®ï¼Œè¿›è¡Œå†…åµŒå¤„ç†
                cited_text = self.insert_grounding_citations(full_text, grounding_metadata)
                yield cited_text
            elif full_text:
                # å¦‚æœæ²¡æœ‰å…ƒæ•°æ®ï¼Œç›´æ¥è¾“å‡ºç¼“å†²çš„æ–‡æœ¬
                yield full_text

            if (
                not finish_reason_received
                and not detected_tool_calls
                and content_yielded
            ):
                logger.warning("Stream ended unexpectedly.")

    async def get_request_stream(
        self,
        messages: list,
        model_name: str,
        tools: Optional[List[dict]] = None,
        __user__: Optional[dict] = None,
        __event_call__: Optional[Callable[[dict], Awaitable[dict]]] = None,
    ) -> AsyncGenerator[str, None]:
        user_valves = (__user__ or self.default).get("valves", self.UserValves())
        api_model = self.valves.api_model

        # 1. è½¬æ¢æ¶ˆæ¯å†å²
        current_messages = []
        for msg in messages:
            role = "user" if msg.get("role") == "user" else "model"
            content = msg.get("content")
            parts = []

            if isinstance(content, str):
                parts.append({"text": content})
            elif isinstance(content, list):
                for part in content:
                    if part.get("type") == "text":
                        parts.append({"text": part.get("text", "")})
                    elif part.get("type") == "image_url":
                        # ç®€åŒ–çš„å›¾ç‰‡å¤„ç†
                        url = part.get("image_url", {}).get("url", "")
                        if "base64," in url:
                            header, data = url.split(",", 1)
                            mime = header.split(":")[1].split(";")[0]
                            parts.append(
                                {"inlineData": {"mimeType": mime, "data": data}}
                            )

            # å¤„ç† OpenWebUI å†å²ä¸­çš„ tool_calls (å¦‚æœæ˜¯å¤šè½®å¯¹è¯)
            # è¿™éƒ¨åˆ†é€šå¸¸æ¯”è¾ƒå¤æ‚ï¼Œå› ä¸º OpenWebUI ä¼ å›çš„å†å²æ ¼å¼å¯èƒ½å·²ç»æ˜¯ OpenAI æ ¼å¼
            # æˆ‘ä»¬éœ€è¦ç¡®ä¿å®ƒèƒ½è¢«æ­£ç¡®æ˜ å°„å› Gemini æ ¼å¼ã€‚
            # æš‚æ—¶å‡è®¾ OpenWebUI ä¼ å…¥çš„ message history ä¸»è¦æ˜¯ textï¼Œå·¥å…·å†å²ç”±æœ¬å‡½æ•°å†…éƒ¨å¾ªç¯ç»´æŠ¤ã€‚

            if parts:
                current_messages.append({"role": role, "parts": parts})

        if current_messages and current_messages[-1]["role"] == "model":
            current_messages.append({"role": "user", "parts": [{"text": "Continue"}]})

        # 2. å·¥å…·é…ç½®
        gemini_tools = [{"googleSearch": {}}]
        if tools:
            converted = self.convert_openai_tools_to_gemini(tools)
            if converted:
                gemini_tools.append({"function_declarations": converted})

        # 3. æ‰§è¡Œå¾ªç¯ (å¤„ç†å·¥å…·è°ƒç”¨)
        MAX_LOOPS = 5
        loop_count = 0

        while loop_count < MAX_LOOPS:
            loop_count += 1

            payload = {
                "contents": current_messages,
                "tools": gemini_tools,
                "generationConfig": {
                    "temperature": user_valves.temperature,
                    "topP": user_valves.top_p,
                    "thinkingConfig": {
                        "includeThoughts": user_valves.include_thoughts,
                        "thinkingBudget": user_valves.thinking_budget,
                    }
                    if "thinking" in api_model or "flash" in api_model
                    else None,  # ç®€å•åˆ¤å®š
                },
            }
            # æ¸…ç† None å€¼
            if not payload["generationConfig"]["thinkingConfig"]:
                del payload["generationConfig"]["thinkingConfig"]

            url = f"/v1beta/models/{api_model}:streamGenerateContent?key={self.valves.api_key}&alt=sse"
            detected_tool_calls = []
            accumulated_text_parts = []  # æœ¬è½®ç”Ÿæˆçš„æ–‡æœ¬

            async with httpx.AsyncClient(
                base_url=self.valves.base_url, timeout=self.valves.timeout
            ) as client:
                if loop_count > 1:
                    await self.emit_status("Processing tool outputs...", done=False)

                try:
                    async with client.stream("POST", url, json=payload) as response:
                        if response.status_code != 200:
                            err = await response.aread()
                            yield f"ğŸš¨ Error {response.status_code}: {err.decode()}"
                            return

                        async for chunk in self.process_stream(
                            response, detected_tool_calls, accumulated_text_parts
                        ):
                            yield chunk
                except Exception as e:
                    yield f"ğŸš¨ Network Error: {e}"
                    return

            # 4. å¦‚æœæ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œç»“æŸ
            if not detected_tool_calls:
                break

            # 5. å¤„ç†å·¥å…·è°ƒç”¨
            if not __event_call__:
                logger.warning("Tool call detected but no handler available.")
                break

            # === å…³é”®ä¿®æ­£ï¼šæ„å»ºæ­£ç¡®çš„å†å²è®°å½• ===

            # A. æ·»åŠ æ¨¡å‹å›åˆ (Model Turn)
            # å¿…é¡»åŒ…å«æœ¬è½®ç”Ÿæˆçš„æ‰€æœ‰æ–‡æœ¬ + å·¥å…·è°ƒç”¨
            model_parts = []
            full_text = "".join(accumulated_text_parts)
            if full_text:
                model_parts.append({"text": full_text})

            for tc in detected_tool_calls:
                model_parts.append({"functionCall": tc})

            current_messages.append({"role": "model", "parts": model_parts})

            # B. æ‰§è¡Œå·¥å…·å¹¶æ·»åŠ å‡½æ•°å›åˆ (Function Turn)
            function_parts = []
            for tc in detected_tool_calls:
                func_name = tc.get("name")
                func_args = tc.get("args", {})

                await self.emit_status(f"ğŸ› ï¸ Calling: {func_name}...", done=False)

                try:
                    # OpenWebUI æœŸæœ› arguments æ˜¯ dict
                    result = await __event_call__(
                        {"name": func_name, "arguments": func_args}
                    )
                    
                    # å°†å·¥å…·è¿”å›å€¼ä½œä¸ºå†…å®¹çš„ä¸€éƒ¨åˆ†è¿”å›
                    if result:
                        yield str(result)

                    # Gemini æœŸæœ› response æ˜¯ä¸€ä¸ª Objectï¼Œä¸èƒ½æ˜¯çº¯å­—ç¬¦ä¸²
                    # å¦‚æœç»“æœæ˜¯å­—ç¬¦ä¸²ï¼ŒåŒ…è£…å®ƒ
                    content_to_send = result
                    if not isinstance(result, (dict, list)):
                        content_to_send = {"result": str(result)}

                    function_parts.append(
                        {
                            "functionResponse": {
                                "name": func_name,
                                "response": content_to_send,
                            }
                        }
                    )
                except Exception as e:
                    logger.error(f"å·¥å…· {func_name} æ‰§è¡Œå¤±è´¥ï¼š{e}")
                    function_parts.append(
                        {
                            "functionResponse": {
                                "name": func_name,
                                "response": {"error": str(e)},
                            }
                        }
                    )

            # æ·»åŠ  Function å“åº”æ¶ˆæ¯
            # v1beta REST API ä¸­ï¼ŒFunction Response çš„ role é€šå¸¸æ˜¯ 'function'
            current_messages.append({"role": "function", "parts": function_parts})

            # å¾ªç¯ç»§ç»­ï¼Œå‘é€åŒ…å«ç»“æœçš„æ–°è¯·æ±‚

    async def pipe(
        self,
        body: dict,
        __user__: Optional[dict] = None,
        __tools__: Optional[List[dict]] = None,
        __event_emitter__: Optional[Callable[[dict], Awaitable[None]]] = None,
        __event_call__: Optional[Callable[[dict], Awaitable[dict]]] = None,
    ) -> AsyncGenerator[str, None]:
        """
        ç®¡é“å…¥å£ã€‚
        """
        self.emitter = __event_emitter__
        user_valves = (__user__ or self.default).get("valves", self.UserValves())
        request_id = str(uuid.uuid4())

        # æå– Open WebUI ä¼ é€’çš„å·¥å…·å®šä¹‰
        # Open WebUI é€šå¸¸åœ¨ body ä¸­ä¼ é€’ 'tools' (OpenAI æ ¼å¼)
        incoming_tools = body.get("tools", [])

        try:
            await self.emit_status("æ­£åœ¨éªŒè¯è¯·æ±‚è´Ÿè½½...", done=False)
            if not isinstance(body, dict):
                error_msg = "âŒ é”™è¯¯ï¼šè¯·æ±‚ä½“å¿…é¡»æ˜¯æœ‰æ•ˆçš„ JSON å¯¹è±¡ã€‚"
                yield error_msg
                await self.emit_status("é”™è¯¯ï¼šæ— æ•ˆçš„è¯·æ±‚ä½“ã€‚", done=True)
                return
            messages = body.get("messages")
            if not messages or not isinstance(messages, list):
                error_msg = "âŒ é”™è¯¯ï¼šè¯·æ±‚ä½“å¿…é¡»åŒ…å«ä¸€ä¸ªæœ‰æ•ˆçš„ 'messages' åˆ—è¡¨ã€‚"
                yield error_msg
                await self.emit_status(
                    "é”™è¯¯ï¼šç¼ºå°‘æˆ–æ— æ•ˆçš„ 'messages' åˆ—è¡¨ã€‚", done=True
                )
                return

            if not self.valves.api_key:
                yield "âŒ Error: API Key not set."
                return

            model_id = body.get("model", self.valves.model_id)

            await self.emit_status(f"æ­£åœ¨ä½¿ç”¨æ¨¡å‹ '{model_id}' å¼€å§‹ç”Ÿæˆ...", done=False)

            stream_had_error = False
            full_response = ""
            async for chunk in self.get_request_stream(
                messages=messages,
                model_name=model_id,
                tools=incoming_tools,
                __user__=__user__,
                __event_call__=__event_call__,
            ):
                if chunk.startswith("ğŸš¨"):
                    stream_had_error = True
                    yield chunk
                    continue

                full_response += chunk

                # è¾“å‡ºå¤„ç†é€»è¾‘ (å—/å­—ç¬¦/å»¶è¿Ÿ)
                if user_valves.block_size > 1:
                    for chunk_part in self.split_html_tags(chunk):
                        if chunk_part.startswith("<") and chunk_part.endswith(">"):
                            yield chunk_part
                        else:
                            for i in range(0, len(chunk_part), user_valves.block_size):
                                block = chunk_part[i : i + user_valves.block_size]
                                if block:
                                    yield block
                                    if user_valves.output_delay > 0:
                                        await asyncio.sleep(user_valves.output_delay)
                elif user_valves.block_size < 0:
                    for chunk_part in self.split_html_tags(chunk):
                        if chunk_part.startswith("<") and chunk_part.endswith(">"):
                            yield chunk_part
                        else:
                            parts = re.split(r"(\s+)", chunk_part)
                            for part in parts:
                                if part:
                                    yield part
                                    if user_valves.output_delay > 0:
                                        await asyncio.sleep(user_valves.output_delay)
                else:
                    skip = False
                    for char in chunk:
                        yield char
                        if char == "<":
                            skip = True
                        elif char == ">":
                            skip = False
                        if skip:
                            continue
                        if user_valves.output_delay > 0:
                            await asyncio.sleep(user_valves.output_delay)

            if not full_response and not stream_had_error:
                logger.warning(f"[{request_id}] å“åº”æµç»“æŸä½†æœªæ”¶åˆ°ä»»ä½•æ–‡æœ¬å†…å®¹ã€‚")
                yield ""

            if not stream_had_error:
                logger.info(f"[{request_id}] å†…å®¹ç”ŸæˆæˆåŠŸä¸”æ— é”™è¯¯ã€‚")
                await self.emit_status("ç”Ÿæˆå®Œæˆã€‚", done=True)
            else:
                logger.warning(f"[{request_id}] å†…å®¹ç”ŸæˆæœŸé—´å‘ç”Ÿé”™è¯¯ã€‚")

        except Exception as e:
            error_msg = f"âŒ ç®¡é“ä¸­å‘ç”Ÿæ„å¤–çš„ç³»ç»Ÿé”™è¯¯ï¼š{e}"
            logger.exception(f"[{request_id}] {error_msg}")
            await self.emit_status(f"è‡´å‘½é”™è¯¯ï¼š{e}", done=True)
            yield error_msg

        logger.info(f"[{request_id}] ç®¡é“å¤„ç†ç»“æŸã€‚")